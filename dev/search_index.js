var documenterSearchIndex = {"docs":
[{"location":"#MPI.jl-1","page":"MPI.jl","title":"MPI.jl","text":"","category":"section"},{"location":"#","page":"MPI.jl","title":"MPI.jl","text":"This is a basic Julia wrapper for the portable message passing system Message Passing Interface (MPI). Inspiration is taken from mpi4py, although we generally follow the C and not the C++ MPI API. (The C++ MPI API is deprecated.)","category":"page"},{"location":"installing/#Installing-1","page":"Installing","title":"Installing","text":"","category":"section"},{"location":"installing/#Unix-systems-(OSX-and-Linux)-1","page":"Installing","title":"Unix systems (OSX and Linux)","text":"","category":"section"},{"location":"installing/#","page":"Installing","title":"Installing","text":"CMake is used to piece together the MPI wrapper. Currently a shared library MPI installation for C and Fortran is required (tested with Open MPI and MPICH). To install MPI.jl using the Julia packaging system, run","category":"page"},{"location":"installing/#","page":"Installing","title":"Installing","text":"Pkg.update()\nPkg.add(\"MPI\")","category":"page"},{"location":"installing/#","page":"Installing","title":"Installing","text":"Alternatively,","category":"page"},{"location":"installing/#","page":"Installing","title":"Installing","text":"Pkg.clone(\"https://github.com/JuliaParallel/MPI.jl.git\")\nPkg.build()","category":"page"},{"location":"installing/#","page":"Installing","title":"Installing","text":"which will build and install the wrapper into $HOME/.julia/vX.Y/MPI.","category":"page"},{"location":"installing/#Platform-specific-notes:-1","page":"Installing","title":"Platform specific notes:","text":"","category":"section"},{"location":"installing/#","page":"Installing","title":"Installing","text":"If you are trying to build on OSX with Homebrew, the necessary Fortran headers are not included in the OpenMPI bottle.  To workaround this you can build OpenMPI from source: brew install --build-from-source openmpi","category":"page"},{"location":"installing/#Overriding-compilers-1","page":"Installing","title":"Overriding compilers","text":"","category":"section"},{"location":"installing/#","page":"Installing","title":"Installing","text":"Currently, MPI.jl relies on CMake for building a few C/Fortran source files needed by the library. Unfortunately, CMake does not follow the PATH variable when determining which compiler to use, which could cause problem if the compiler you want to use does not reside in a standard directory like /usr/bin. You can override CMake's detection of the compiler by specifying the environment variables CC, CXX, and FC on the command line. The following example forces the compilation process to use the compilers found in the path:","category":"page"},{"location":"installing/#","page":"Installing","title":"Installing","text":"CC=$(which gcc) CXX=$(which g++) FC=$(which gfortran) julia -e 'Pkg.add(\"MPI\")'","category":"page"},{"location":"installing/#Overriding-the-auto-detected-MPI-version-1","page":"Installing","title":"Overriding the auto-detected MPI version","text":"","category":"section"},{"location":"installing/#","page":"Installing","title":"Installing","text":"It may be that CMake selects the wrong MPI version, or that CMake fails to correctly detect and configure your MPI implementation. You can override CMake's mechanism by setting certain environment variables:","category":"page"},{"location":"installing/#","page":"Installing","title":"Installing","text":"JULIA_MPI_C_LIBRARIES\nJULIA_MPI_Fortran_INCLUDE_PATH\nJULIA_MPI_Fortran_LIBRARIES","category":"page"},{"location":"installing/#","page":"Installing","title":"Installing","text":"This will set MPI_C_LIBRARIES, MPI_Fortran_INCLUDE_PATH, and MPI_Fortran_LIBRARIES when calling CMake as described in its FindMPI module. You can set these variables either in your shell startup file, or e.g. via your ~/.juliarc file. Here is an example:","category":"page"},{"location":"installing/#","page":"Installing","title":"Installing","text":"ENV[\"JULIA_MPI_C_LIBRARIES\"] = \"-L/opt/local/lib/openmpi-gcc5 -lmpi\"\nENV[\"JULIA_MPI_Fortran_INCLUDE_PATH\"] = \"/opt/local/include\"\nENV[\"JULIA_MPI_Fortran_LIBRARIES\"] = \"-L/opt/local/lib/openmpi-gcc5 -lmpi_usempif08 -lmpi_mpifh -lmpi\"","category":"page"},{"location":"installing/#","page":"Installing","title":"Installing","text":"You can set other configuration variables as well (by adding a JULIA_ prefix); the full list of variables currently supported is","category":"page"},{"location":"installing/#","page":"Installing","title":"Installing","text":"MPI_C_COMPILER\nMPI_C_COMPILE_FLAGS\nMPI_C_INCLUDE_PATH\nMPI_C_LINK_FLAGS\nMPI_C_LIBRARIES\nMPI_Fortran_COMPILER\nMPI_Fortran_COMPILE_FLAGS\nMPI_Fortran_INCLUDE_PATH\nMPI_Fortran_LINK_FLAGS\nMPI_Fortran_LIBRARIES\nMPI_INCLUDE_PATH\nMPI_LIBRARIES","category":"page"},{"location":"installing/#Windows-1","page":"Installing","title":"Windows","text":"","category":"section"},{"location":"installing/#","page":"Installing","title":"Installing","text":"You need to install the Microsoft MPI runtime on your system (the SDK is not required). Then simply add/build the MPI.jl package with","category":"page"},{"location":"installing/#","page":"Installing","title":"Installing","text":"Pkg.update()\nPkg.add(\"MPI\")","category":"page"},{"location":"installing/#","page":"Installing","title":"Installing","text":"If you would like to wrap an MPI function on Windows, keep in mind you may need to add its signature to src/win_mpiconstants.jl.","category":"page"},{"location":"usage/#Usage-1","page":"Usage","title":"Usage","text":"","category":"section"},{"location":"usage/#MPI-only-mode-1","page":"Usage","title":"MPI-only mode","text":"","category":"section"},{"location":"usage/#","page":"Usage","title":"Usage","text":"To run a Julia script with MPI, first make sure that using MPI or import MPI is included at the top of your script. You should then be able to run the MPI job as expected, e.g. to run examples/01-hello.jl,","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"mpirun -np 3 julia 01-hello.jl","category":"page"},{"location":"usage/#MPI-and-Julia-parallel-constructs-together-1","page":"Usage","title":"MPI and Julia parallel constructs together","text":"","category":"section"},{"location":"usage/#","page":"Usage","title":"Usage","text":"In order for MPI calls to be made from a Julia cluster, it requires the use of MPIManager, a cluster manager that will start the julia workers using mpirun","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"It has three modes of operation","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"Only worker processes execute MPI code. The Julia master process executes outside of and is not part of the MPI cluster. Free bi-directional TCP/IP connectivity is required between all processes\nAll processes (including Julia master) are part of both the MPI as well as Julia cluster. Free bi-directional TCP/IP connectivity is required between all processes.\nAll processes are part of both the MPI as well as Julia cluster. MPI is used as the transport for julia messages. This is useful on environments which do not allow TCP/IP connectivity between worker processes","category":"page"},{"location":"usage/#MPIManager:-only-workers-execute-MPI-code-1","page":"Usage","title":"MPIManager: only workers execute MPI code","text":"","category":"section"},{"location":"usage/#","page":"Usage","title":"Usage","text":"An example is provided in examples/05-juliacman.jl. The julia master process is NOT part of the MPI cluster. The main script should be launched directly, MPIManager internally calls mpirun to launch julia/MPI workers. All the workers started via MPIManager will be part of the MPI cluster.","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"MPIManager(;np=Sys.CPU_THREADS, mpi_cmd=false, launch_timeout=60.0)","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"If not specified, mpi_cmd defaults to mpirun -np $np stdout from the launched workers is redirected back to the julia session calling addprocs via a TCP connection. Thus the workers must be able to freely connect via TCP to the host session. The following lines will be typically required on the julia master process to support both julia and MPI:","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"# to import MPIManager\nusing MPI\n\n# need to also import Distributed to use addprocs()\nusing Distributed\n\n# specify, number of mpi workers, launch cmd, etc.\nmanager=MPIManager(np=4)\n\n# start mpi workers and add them as julia workers too.\naddprocs(manager)","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"To execute code with MPI calls on all workers, use @mpi_do.","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"@mpi_do manager expr executes expr on all processes that are part of manager.","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"For example:","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"@mpi_do manager begin\n  comm=MPI.COMM_WORLD\n  println(\"Hello world, I am $(MPI.Comm_rank(comm)) of $(MPI.Comm_size(comm))\"))\nend","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"executes on all MPI workers belonging to manager only","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"examples/05-juliacman.jl is a simple example of calling MPI functions on all workers interspersed with Julia parallel methods.","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"This should be run without mpirun:","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"julia 05-juliacman.jl","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"A single instation of MPIManager can be used only once to launch MPI workers (via addprocs). To create multiple sets of MPI clusters, use separate, distinct MPIManager objects.","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"procs(manager::MPIManager) returns a list of julia pids belonging to manager mpiprocs(manager::MPIManager) returns a list of MPI ranks belonging to manager","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"Fields j2mpi and mpi2j of MPIManager are associative collections mapping julia pids to MPI ranks and vice-versa.","category":"page"},{"location":"usage/#MPIManager:-TCP/IP-transport-all-processes-execute-MPI-code-1","page":"Usage","title":"MPIManager: TCP/IP transport - all processes execute MPI code","text":"","category":"section"},{"location":"usage/#","page":"Usage","title":"Usage","text":"Useful on environments which do not allow TCP connections outside of the cluster","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"An example is in examples/06-cman-transport.jl:","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"mpirun -np 5 julia 06-cman-transport.jl TCP","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"This launches a total of 5 processes, mpi rank 0 is the julia pid 1. mpi rank 1 is julia pid 2 and so on.","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"The program must call MPI.start(TCP_TRANSPORT_ALL) with argument TCP_TRANSPORT_ALL. On mpi rank 0, it returns a manager which can be used with @mpi_do On other processes (i.e., the workers) the function does not return","category":"page"},{"location":"usage/#MPIManager:-MPI-transport-all-processes-execute-MPI-code-1","page":"Usage","title":"MPIManager: MPI transport - all processes execute MPI code","text":"","category":"section"},{"location":"usage/#","page":"Usage","title":"Usage","text":"MPI.start must be called with option MPI_TRANSPORT_ALL to use MPI as transport.","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"mpirun -np 5 julia 06-cman-transport.jl MPI","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"will run the example using MPI as transport.","category":"page"},{"location":"usage/#Finalizers-1","page":"Usage","title":"Finalizers","text":"","category":"section"},{"location":"usage/#","page":"Usage","title":"Usage","text":"In order to ensure MPI routines are called in the correct order at finalization time, MPI.jl maintains a reference count. If you define an object that needs to call an MPI routine during its finalization, you should call MPI.refcount_inc() when it is initialized, and MPI.refcount_dec() in its finalizer (after the relevant MPI call).","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"For example","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"mutable struct MyObject\n    ...\n    function MyObject(args...)\n        obj = new(args...)\n        # MPI call to create object\n        refcount_inc()\n        finalizer(obj) do x\n            # MPI call to free object\n            refcount_dec()\n        end\n        return obj\n    end\nend","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"","category":"page"},{"location":"functions/#Function-reference-1","page":"Function reference","title":"Function reference","text":"","category":"section"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"The following functions are currently wrapped, with the convention: MPI_Fun => MPI.Fun","category":"page"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"Constants like MPI_SUM are wrapped as MPI.SUM.   Note also that arbitrary Julia functions f(x,y) can be passed as reduction operations to the MPI Allreduce and Reduce functions.","category":"page"},{"location":"functions/#Communicators-1","page":"Function reference","title":"Communicators","text":"","category":"section"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"Julia interfaces to the Fortran versions of the MPI functions. Since the C and Fortran communicators are different, if a C communicator is required (e.g., to interface with a C library), this can be achieved with the Fortran to C communicator conversion:","category":"page"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"juliacomm = MPI.COMM_WORLD\nccomm = MPI.CComm(juliacomm)","category":"page"},{"location":"functions/#Administrative-functions-1","page":"Function reference","title":"Administrative functions","text":"","category":"section"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"Julia Function (assuming import MPI) Fortran Function\nMPI.Abort MPI_Abort\nMPI.Comm_dup MPI_Comm_dup\nMPI.Comm_free MPI_Comm_free\nMPI.Comm_get_parent MPI_Comm_get_parent\nMPI.Comm_rank MPI_Comm_rank\nMPI.Comm_size MPI_Comm_size\nMPI.Comm_spawn MPI_Comm_spawn\nMPI.Finalize MPI_Finalize\nMPI.Finalized MPI_Finalized\nMPI.Get_address MPI_Get_address\nMPI.Init MPI_Init\nMPI.Initialized MPI_Initialized\nMPI.Intercomm_merge MPI_Intercomm_merge\nMPI.mpitype MPI_Type_create_struct/MPI_Type_commit","category":"page"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"note: Note\nmpitype is not strictly a wrapper for MPI_Type_create_struct and MPI_Type_commit, it also is an accessor for previously created types.","category":"page"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"MPI.Abort\nMPI.Comm_dup\nMPI.Comm_free\nMPI.Comm_get_parent\nMPI.Comm_rank\nMPI.Comm_size\nMPI.Comm_spawn\nMPI.Finalize\nMPI.Finalized\nMPI.Get_address\nMPI.Init\nMPI.Initialized\nMPI.Intercomm_merge\nMPI.mpitype\nMPI.refcount_inc()\nMPI.refcount_dec()","category":"page"},{"location":"functions/#MPI.Abort","page":"Function reference","title":"MPI.Abort","text":"Abort(comm::Comm, errcode::Integer)\n\nMake a “best attempt” to abort all tasks in the group of comm. This function does not require that the invoking environment take any action with the error code. However, a Unix or POSIX environment should handle this as a return errorcode from the main program.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Comm_rank","page":"Function reference","title":"MPI.Comm_rank","text":"Comm_rank(comm:Comm)\n\nThe rank of the process in the particular communicator’s group. \n\nReturns an integer in the range 0:MPI.Comm_size()-1.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Comm_size","page":"Function reference","title":"MPI.Comm_size","text":"Comm_size(comm:Comm)\n\nThe number of processes involved in communicator.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Finalize","page":"Function reference","title":"MPI.Finalize","text":"Finalize()\n\nMarks MPI state for cleanup. This should be called after Init, at most once, and no further MPI calls (other than Initialized or Finalized) should be made after it is called.\n\nNote that this does not correspond exactly to MPI_FINALIZE in the MPI specification. In particular:\n\nIt may not finalize MPI immediately. Julia will wait until all MPI-related objects are garbage collected before finalizing MPI. As a result, Finalized() may return false after Finalize() has been called. See Finalizers for more details.\nIt is optional: Init will automatically insert a hook to finalize MPI when Julia exits.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Finalized","page":"Function reference","title":"MPI.Finalized","text":"Finalized()\n\nReturns true if MPI.Finalize has completed, false otherwise. \n\nIt is safe to call before MPI.Init and after MPI.Finalize.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Init","page":"Function reference","title":"MPI.Init","text":"Init()\n\nInitialize MPI in the current process.\n\nAll MPI programs must contain exactly one call to MPI.Init().\n\nThe only MPI functions that may be called before MPI.Init() are MPI.Initialized and MPI.Finalized.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Initialized","page":"Function reference","title":"MPI.Initialized","text":"Initialized()\n\nReturns true if MPI.Init has been called, false otherwise. \n\nIt is unaffected by MPI.Finalize, and is one of the few functions that may be called before MPI.Init.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.refcount_inc-Tuple{}","page":"Function reference","title":"MPI.refcount_inc","text":"refcount_inc()\n\nIncrement the MPI reference counter. This should be called at initialization of any object which calls an MPI routine in its finalizer. A matching refcount_dec should be added to the finalizer.\n\nFor more details, see Finalizers.\n\n\n\n\n\n","category":"method"},{"location":"functions/#MPI.refcount_dec-Tuple{}","page":"Function reference","title":"MPI.refcount_dec","text":"refcount_dec()\n\nDecrement the MPI reference counter. This should be added after an MPI call in an object finalizer, with a matching refcount_inc when the object is initialized.\n\nFor more details, see Finalizers.\n\n\n\n\n\n","category":"method"},{"location":"functions/#Point-to-point-communication-1","page":"Function reference","title":"Point-to-point communication","text":"","category":"section"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"Julia Function (assuming import MPI) Fortran Function\nMPI.Cancel! MPI_Cancel\nMPI.Get_count MPI_Get_count\nMPI.Iprobe MPI_Iprobe\nMPI.Irecv! MPI_Irecv\nMPI.Isend MPI_Isend\nMPI.Probe MPI_Probe\nMPI.Recv! MPI_Recv\nMPI.Send MPI_Send\nMPI.Test! MPI_Test\nMPI.Testall! MPI_Testall\nMPI.Testany! MPI_Testany\nMPI.Testsome! MPI_Testsome\nMPI.Wait! MPI_Wait\nMPI.Waitall! MPI_Waitall\nMPI.Waitany! MPI_Waitany\nMPI.Waitsome! MPI_Waitsome","category":"page"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"MPI.Cancel!\nMPI.Get_count\nMPI.Iprobe\nMPI.Irecv!\nMPI.Isend\nMPI.Probe\nMPI.Recv!\nMPI.Send\nMPI.Test!\nMPI.Testall!\nMPI.Testany!\nMPI.Testsome!\nMPI.Wait!\nMPI.Waitall!\nMPI.Waitany!\nMPI.Waitsome!","category":"page"},{"location":"functions/#MPI.Irecv!","page":"Function reference","title":"MPI.Irecv!","text":"Irecv!(buf::MPIBuffertype{T}, count::Integer, src::Integer, tag::Integer,\n       comm::Comm) where T\n\nStarts a nonblocking receive of up to count elements into buf from MPI rank src of communicator comm using with the message tag tag\n\nReturns the communication Request for the nonblocking receive.\n\n\n\n\n\nIrecv!(buf::Array{T}, src::Integer, tag::Integer, comm::Comm) where T\n\nStarts a nonblocking receive into buf from MPI rank src of communicator comm using with the message tag tag\n\nReturns the communication Request for the nonblocking receive.\n\n\n\n\n\nIrecv!(buf::SubArray{T}, src::Integer, tag::Integer, comm::Comm) where T\n\nStarts a nonblocking receive into SubArray buf from MPI rank src of communicator comm using with the message tag tag. Note that buf must be contiguous.\n\nReturns the communication Request for the nonblocking receive.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Isend","page":"Function reference","title":"MPI.Isend","text":"Isend(buf::MPIBuffertype{T}, count::Integer, dest::Integer, tag::Integer,\n      comm::Comm) where T\n\nStarts a nonblocking send of count elements of buf to MPI rank dest of communicator comm using with the message tag tag\n\nReturns the commication Request for the nonblocking send.\n\n\n\n\n\nIsend(buf::Array{T}, dest::Integer, tag::Integer, comm::Comm) where T\n\nStarts a nonblocking send of buf to MPI rank dest of communicator comm using with the message tag tag\n\nReturns the commication Request for the nonblocking send.\n\n\n\n\n\nIsend(buf::SubArray{T}, dest::Integer, tag::Integer, comm::Comm) where T\n\nStarts a nonblocking send of SubArray buf to MPI rank dest of communicator comm using with the message tag tag. Note that the buf must be contiguous.\n\nReturns the commication Request for the nonblocking send.\n\n\n\n\n\nIsend(obj::T, dest::Integer, tag::Integer, comm::Comm) where T\n\nStarts a nonblocking send of obj to MPI rank dest of communicator comm using with the message tag tag.\n\nReturns the commication Request for the nonblocking send.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Recv!","page":"Function reference","title":"MPI.Recv!","text":"Recv!(buf::MPIBuffertype{T}, count::Integer, src::Integer, tag::Integer,\n      comm::Comm) where T\n\nCompletes a blocking receive of up to count elements into buf from MPI rank src of communicator comm using with the message tag tag\n\nReturns the Status of the receive\n\n\n\n\n\nRecv!(buf::Array{T}, src::Integer, tag::Integer, comm::Comm) where T\n\nCompletes a blocking receive into buf from MPI rank src of communicator comm using with the message tag tag\n\nReturns the Status of the receive\n\n\n\n\n\nRecv!(buf::SubArray{T}, src::Integer, tag::Integer, comm::Comm) where T\n\nCompletes a blocking receive into SubArray buf from MPI rank src of communicator comm using with the message tag tag. Note that buf must be contiguous.\n\nReturns the Status of the receive\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Send","page":"Function reference","title":"MPI.Send","text":"Send(buf::MPIBuffertype{T}, count::Integer, dest::Integer, tag::Integer,\n     comm::Comm) where T\n\nComplete a blocking send of count elements of buf to MPI rank dest of communicator comm using with the message tag tag\n\n\n\n\n\nSend(buf::Array{T}, dest::Integer, tag::Integer, comm::Comm) where T\n\nComplete a blocking send of buf to MPI rank dest of communicator comm using with the message tag tag\n\n\n\n\n\nSend(buf::SubArray{T}, dest::Integer, tag::Integer, comm::Comm) where T\n\nComplete a blocking send of SubArray buf to MPI rank dest of communicator comm using with the message tag tag. Note that the buf must be contiguous.\n\n\n\n\n\nSend(obj::T, dest::Integer, tag::Integer, comm::Comm) where T\n\nComplete s blocking send of obj to MPI rank dest of communicator comm using with the message tag tag.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Wait!","page":"Function reference","title":"MPI.Wait!","text":"Wait!(req::Request)\n\nWait on the request req to be complete. Returns the Status of the request.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Waitall!","page":"Function reference","title":"MPI.Waitall!","text":"Waitall!(reqs::Array{Request,1})\n\nWait on all the requests in the array reqs to be complete. Returns an arrays of the all the requests statuses.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Waitany!","page":"Function reference","title":"MPI.Waitany!","text":"Waitany!(reqs::Array{Request,1})\n\nWait on any the requests in the array reqs to be complete. Returns the index of the completed request and its Status as a tuple.\n\n\n\n\n\n","category":"function"},{"location":"functions/#Collective-communication-1","page":"Function reference","title":"Collective communication","text":"","category":"section"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"Non-Allocating Julia Function Allocating Julia Function Fortran Function Supports MPI_IN_PLACE\nMPI.Allgather! MPI.Allgather MPI_Allgather ✅\nMPI.Allgatherv! MPI.Allgatherv MPI_Allgatherv ✅\nMPI.Allreduce! MPI.Allreduce MPI_Allreduce ✅\nMPI.Alltoall! MPI.Alltoall MPI_Alltoall ✅\nMPI.Alltoallv! MPI.Alltoallv MPI_Alltoallv ❌\n– MPI.Barrier MPI_Barrier ❌\nMPI.Bcast! MPI.Bcast! MPI_Bcast ❌\n– MPI.Exscan MPI_Exscan ❌\nMPI.Gather! MPI.Gather MPI_Gather Gather_in_place!\nMPI.Gatherv! MPI.Gatherv MPI_Gatherv Gatherv_in_place!\nMPI.Reduce! MPI.Reduce MPI_Reduce Reduce_in_place!\nMPI.Scan MPI.Scan MPI_Scan missing\nMPI.Scatter! MPI.Scatter MPI_Scatter Scatter_in_place!\nMPI.Scatterv! MPI.Scatterv MPI_Scatterv Scatterv_in_place!","category":"page"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"The non-allocating Julia functions map directly to the corresponding MPI operations, after asserting that the size of the output buffer is sufficient to store the result.","category":"page"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"The allocating Julia functions allocate an output buffer and then call the non-allocating method.","category":"page"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"All-to-all collective communications support in place operations by passing MPI.IN_PLACE with the same syntax documented by MPI. One-to-All communications support it by calling the function *_in_place!, calls the MPI functions with the right syntax on root and non root process.","category":"page"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"MPI.Allgather!\nMPI.Allgather\nMPI.Allgatherv!\nMPI.Allgatherv\nMPI.Allreduce!\nMPI.Allreduce\nMPI.Alltoall!\nMPI.Alltoall\nMPI.Alltoallv!\nMPI.Alltoallv\nMPI.Barrier\nMPI.Bcast!\nMPI.Bcast!\nMPI.Exscan\nMPI.Gather!\nMPI.Gather\nMPI.Gather_in_place!\nMPI.Gatherv!\nMPI.Gatherv\nMPI.Gatherv_in_place!\nMPI.Reduce!\nMPI.Reduce\nMPI.Reduce_in_place!\nMPI.Scan\nMPI.Scan\nMPI.Scatter!\nMPI.Scatter\nMPI.Scatter_in_place!\nMPI.Scatterv!\nMPI.Scatterv\nMPI.Scatterv_in_place!","category":"page"},{"location":"functions/#MPI.Allgather!","page":"Function reference","title":"MPI.Allgather!","text":"Allgather!(sendbuf, recvbuf, count, comm)\n\nEach process sends the first count elements of sendbuf to the other processes, who store the results in rank order into recvbuf.\n\nIf sendbuf==MPI.IN_PLACE the input data is assumed to be in the area of recvbuf where the process would receive it's own contribution.\n\n\n\n\n\nAllgather!(buf, count, comm)\n\nEquivalent to Allgather!(MPI.IN_PLACE, buf, count, comm).\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Allgather","page":"Function reference","title":"MPI.Allgather","text":"Allgather(sendbuf, count, comm)\n\nEach process sends the first count elements of sendbuf to the other processes, who store the results in rank order allocating the output buffer.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Allgatherv!","page":"Function reference","title":"MPI.Allgatherv!","text":"Allgatherv!(sendbuf, recvbuf, counts, comm)\n\nEach process sends the first counts[rank] elements of the buffer sendbuf to all other process. Each process stores the received data in rank order in the buffer recvbuf.\n\nif sendbuf==MPI.IN_PLACE every process takes the data to be sent is taken from the interval of recvbuf where it would store it's own data.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Allgatherv","page":"Function reference","title":"MPI.Allgatherv","text":"Allgatherv(sendbuf, counts, comm)\n\nEach process sends the first counts[rank] elements of the buffer sendbuf to all other process. Each process allocates an output buffer and stores the received data in rank order.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Allreduce!","page":"Function reference","title":"MPI.Allreduce!","text":"Allreduce!(sendbuf, recvbuf[, count=length(sendbuf)], op, comm)\n\nPerforms op reduction on the first count elements of the buffer sendbuf storing the result in the recvbuf of all processes in the group.\n\nAll-reduce is equivalent to a Reduce! operation followed by a Bcast!, but can lead to better performance.\n\nIf sendbuf==MPI.IN_PLACE the data is read from recvbuf and then overwritten with the results.\n\nTo handle allocation of the output buffer, see Allreduce.\n\n\n\n\n\nAllreduce!(buf, op, comm)\n\nPerforms op reduction in place on the buffer sendbuf, overwriting it with the results on all the processes in the group.\n\nEquivalent to calling Allreduce!(MPI.IN_PLACE, buf, op, comm)\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Allreduce","page":"Function reference","title":"MPI.Allreduce","text":"Allreduce(sendbuf, op, comm)\n\nPerforms op reduction on the buffer sendbuf, allocating and returning the output buffer in all processes of the group.\n\nTo specify the output buffer or perform the operation in pace, see Allreduce!.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Alltoall!","page":"Function reference","title":"MPI.Alltoall!","text":"Alltoall!(sendbuf, recvbuf, count, comm)\n\nEvery process divides the buffer sendbuf into Comm_size(comm) chunks of length count, sending the j-th chunk to the j-th process. Every process stores the data received from the j-th process in the j-th chunk of the buffer recvbuf.\n\nrank    send buf                        recv buf\n----    --------                        --------\n 0      a,b,c,d,e,f       Alltoall      a,b,A,B,α,β\n 1      A,B,C,D,E,F  ---------------->  c,d,C,D,γ,ψ\n 2      α,β,γ,ψ,η,ν                     e,f,E,F,η,ν\n\nIf sendbuf==MPI.IN_PLACE, data is sent from the recvbuf and then overwritten.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Alltoall","page":"Function reference","title":"MPI.Alltoall","text":"Alltoall(sendbuf, count, comm)\n\nEvery process divides the buffer sendbuf into Comm_size(comm) chunks of length count, sending the j-th chunk to the j-th process. Every process allocates the output buffer and stores the data received from the j-th process in the j-th chunk.\n\nrank    send buf                        recv buf\n----    --------                        --------\n 0      a,b,c,d,e,f       Alltoall      a,b,A,B,α,β\n 1      A,B,C,D,E,F  ---------------->  c,d,C,D,γ,ψ\n 2      α,β,γ,ψ,η,ν                     e,f,E,F,η,ν\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Alltoallv!","page":"Function reference","title":"MPI.Alltoallv!","text":"Alltoallv!(sendbuf::T, recvbuf::T, scounts, rcounts, comm)\n\nMPI.IN_PLACE is not supported for this operation.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Barrier","page":"Function reference","title":"MPI.Barrier","text":"Barrier(comm::Comm)\n\nBlocks until comm is synchronized.\n\nIf comm is an intracommunicator, then it blocks until all members of the group have called it.\n\nIf comm is an intercommunicator, then it blocks until all members of the other group have called it.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Bcast!","page":"Function reference","title":"MPI.Bcast!","text":"Bcast!(buf[, count=length(buf)], root, comm::Comm)\n\nBroadcast the first count elements of the buffer buf from root to all processes.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Gather!","page":"Function reference","title":"MPI.Gather!","text":"Gather!(sendbuf, recvbuf, count, root, comm)\n\nEach process sends the first count elements of the buffer sendbuf to the root process. The root process stores elements in rank order in the buffer buffer recvbuf.\n\ncount should be the same for all processes. If the number of elements varies between processes, use Gatherv! instead.\n\nTo perform the reduction in place refer to Gather_in_place!.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Gather","page":"Function reference","title":"MPI.Gather","text":"Gather(sendbuf[, count=length(sendbuf)], root, comm)\n\nEach process sends the first count elements of the buffer sendbuf to the root process. The root allocates the output buffer and stores elements in rank order.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Gather_in_place!","page":"Function reference","title":"MPI.Gather_in_place!","text":"Gather_in_place!(buf, count, root, comm)\n\nEach process sends the first count elements of the buffer buf to the root process. The root process stores elements in rank order in the buffer buffer buf, sending no data to itself.\n\nThis is functionally equivalent to calling\n\nif root == MPI.Comm_rank(comm)\n    Gather!(MPI.IN_PLACE, buf, count, root, comm)\nelse\n    Gather!(buf, C_NULL, count, root, comm)\nend\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Gatherv!","page":"Function reference","title":"MPI.Gatherv!","text":"Gatherv!(sendbuf, recvbuf, counts, root, comm)\n\nEach process sends the first counts[rank] elements of the buffer sendbuf to the root process. The root stores elements in rank order in the buffer recvbuf.\n\nTo perform the reduction in place refer to Gatherv_in_place!.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Gatherv","page":"Function reference","title":"MPI.Gatherv","text":"Gatherv(sendbuf, counts, root, comm)\n\nEach process sends the first counts[rank] elements of the buffer sendbuf to the root process. The root allocates the output buffer and stores elements in rank order.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Gatherv_in_place!","page":"Function reference","title":"MPI.Gatherv_in_place!","text":"Gatherv_in_place!(buf, counts, root, comm)\n\nEach process sends the first counts[rank] elements of the buffer buf to the root process. The root allocates the output buffer and stores elements in rank order.\n\nThis is functionally equivalent to calling\n\nif root == MPI.Comm_rank(comm)\n    Gatherv!(MPI.IN_PLACE, buf, counts, root, comm)\nelse\n    Gatherv!(buf, C_NULL, counts, root, comm)\nend\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Reduce!","page":"Function reference","title":"MPI.Reduce!","text":"Reduce!(sendbuf, recvbuf[, count=length(sendbuf)], op, root, comm)\n\nPerforms op reduction on the first count elements of the  buffer sendbuf and stores the result in recvbuf on the process of rank root.\n\nOn non-root processes recvbuf is ignored.\n\nTo perform the reduction in place, see Reduce_in_place!.\n\nTo handle allocation of the output buffer, see Reduce.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Reduce","page":"Function reference","title":"MPI.Reduce","text":"Reduce(sendbuf, count, op, root, comm)\n\nPerforms op reduction on the buffer sendbuf and stores the result in an output buffer allocated on the process of rank root. An empty array will be returned on all other processes.\n\nTo specify the output buffer, see Reduce!.\n\nTo perform the reduction in place, see Reduce_in_place!.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Reduce_in_place!","page":"Function reference","title":"MPI.Reduce_in_place!","text":"Reduce_in_place!(buf, count, op, root, comm)\n\nPerforms op reduction on the first count elements of the buffer buf and stores the result on buf of the root process in the group.\n\nThis is equivalent to calling\n\nif root == MPI.Comm_rank(comm)\n    Reduce!(MPI.IN_PLACE, buf, count, root, comm)\nelse\n    Reduce!(buf, C_NULL, count, root, comm)\nend\n\nTo handle allocation of the output buffer, see Reduce.\n\nTo specify a separate output buffer, see Reduce!.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Scatter!","page":"Function reference","title":"MPI.Scatter!","text":"Scatter!(sendbuf, recvbuf, count, root, comm)\n\nSplits the buffer sendbuf in the root process into Comm_size(comm) chunks and sends the j-th chunk to the process of rank j into the recvbuf buffer, which must be of length at least count.\n\ncount should be the same for all processes. If the number of elements varies between processes, use Scatter! instead.\n\nTo perform the reduction in place, see Scatter_in_place!.\n\nTo handle allocation of the output buffer, see Scatter.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Scatter","page":"Function reference","title":"MPI.Scatter","text":"Scatter(sendbuf, count, root, comm)\n\nSplits the buffer sendbuf in the root process into Comm_size(comm) chunks and sends the j-th chunk to the process of rank j, allocating the output buffer.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Scatter_in_place!","page":"Function reference","title":"MPI.Scatter_in_place!","text":"Scatter_in_place!(buf, count, root, comm)\n\nSplits the buffer buf in the root process into Comm_size(comm) chunks and sends the j-th chunk to the process of rank j. No data is sent to the root process.\n\nThis is functionally equivalent to calling\n\nif root == MPI.Comm_rank(comm)\n    Scatter!(buf, MPI.IN_PLACE, count, root, comm)\nelse\n    Scatter!(C_NULL, buf, count, root, comm)\nend\n\nTo specify a separate output buffer, see Scatter!.\n\nTo handle allocation of the output buffer, see Scatter.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Scatterv!","page":"Function reference","title":"MPI.Scatterv!","text":"Scatterv!(sendbuf, recvbuf, counts, root, comm)\n\nSplits the buffer sendbuf in the root process into Comm_size(comm) chunks of length counts[j] and sends the j-th chunk to the process of rank j into the recvbuf buffer, which must be of length at least count.\n\nTo perform the reduction in place refer to Scatterv_in_place!.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Scatterv","page":"Function reference","title":"MPI.Scatterv","text":"Scatterv(sendbuf, counts, root, comm)\n\nSplits the buffer sendbuf in the root process into Comm_size(comm) chunks of length counts[j] and sends the j-th chunk to the process of rank j, which allocates the output buffer\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Scatterv_in_place!","page":"Function reference","title":"MPI.Scatterv_in_place!","text":"Scatterv_in_place(buf, counts, root, comm)\n\nSplits the buffer buf in the root process into Comm_size(comm) chunks of length counts[j] and sends the j-th chunk to the process of rank j into the buf buffer, which must be of length at least count. The root process sends nothing to itself.\n\nThis is functionally equivalent to calling\n\nif root == MPI.Comm_rank(comm)\n    Scatterv!(buf, MPI.IN_PLACE, counts, root, comm)\nelse\n    Scatterv!(C_NULL, buf, counts, root, comm)\nend\n\n\n\n\n\n","category":"function"},{"location":"functions/#One-sided-communication-1","page":"Function reference","title":"One-sided communication","text":"","category":"section"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"Julia Function (assuming import MPI) Fortran Function\nMPI.Win_create MPI_Win_create\nMPI.Win_create_dynamic MPI_Win_create_dynamic\nMPI.Win_allocate_shared MPI_Win_allocate_shared\nMPI.Win_shared_query MPI_Win_shared_query\nMPI.Win_attach MPI_Win_attach\nMPI.Win_detach MPI_Win_detach\nMPI.Win_fence MPI_Win_fence\nMPI.Win_flush MPI_Win_flush\nMPI.Win_free MPI_Win_free\nMPI.Win_sync MPI_Win_sync\nMPI.Win_lock MPI_Win_lock\nMPI.Win_unlock MPI_Win_unlock\nMPI.Get MPI_Get\nMPI.Put MPI_Put\nMPI.Fetch_and_op MPI_Fetch_and_op\nMPI.Accumulate MPI_Accumulate\nMPI.Get_accumulate MPI_Get_accumulate","category":"page"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"MPI.Win_create\nMPI.Win_create_dynamic\nMPI.Win_allocate_shared\nMPI.Win_shared_query\nMPI.Win_attach\nMPI.Win_detach\nMPI.Win_fence\nMPI.Win_flush\nMPI.Win_free\nMPI.Win_sync\nMPI.Win_lock\nMPI.Win_unlock\nMPI.Get\nMPI.Put\nMPI.Fetch_and_op\nMPI.Accumulate\nMPI.Get_accumulate","category":"page"},{"location":"functions/#Info-objects-1","page":"Function reference","title":"Info objects","text":"","category":"section"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"MPI.Info\nMPI.infoval","category":"page"},{"location":"functions/#MPI.Info","page":"Function reference","title":"MPI.Info","text":"Info <: AbstractDict{Symbol,String}\n\nMPI.Info objects store key-value pairs, and are typically used for passing optional arguments to MPI functions.\n\nUsage\n\nThese will typically be hidden from user-facing APIs by splatting keywords, e.g.\n\nfunction f(args...; kwargs...)\n    info = Info(kwargs...)\n    # pass `info` object to `ccall`\nend\n\nFor manual usage, Info objects act like Julia Dict objects:\n\ninfo = Info(init=true) # keyword argument is required\ninfo[key] = value\nx = info[key]\ndelete!(info, key)\n\nIf init=false is used in the costructor (the default), a \"null\" Info object will be returned: no keys can be added to such an object.\n\n\n\n\n\n","category":"type"},{"location":"functions/#MPI.infoval","page":"Function reference","title":"MPI.infoval","text":"infoval(x)\n\nConvert Julia object x to a string representation for storing in an Info object.\n\nThe MPI specification allows passing strings, Boolean values, integers, and lists.\n\n\n\n\n\n","category":"function"}]
}
