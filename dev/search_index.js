var documenterSearchIndex = {"docs":
[{"location":"environment/#Environment-1","page":"Environment","title":"Environment","text":"","category":"section"},{"location":"environment/#Functions-1","page":"Environment","title":"Functions","text":"","category":"section"},{"location":"environment/#","page":"Environment","title":"Environment","text":"MPI.Abort\nMPI.Init\nMPI.Initialized\nMPI.Finalize\nMPI.Finalized\nMPI.refcount_inc\nMPI.refcount_dec","category":"page"},{"location":"environment/#MPI.Abort","page":"Environment","title":"MPI.Abort","text":"Abort(comm::Comm, errcode::Integer)\n\nMake a “best attempt” to abort all tasks in the group of comm. This function does not require that the invoking environment take any action with the error code. However, a Unix or POSIX environment should handle this as a return errorcode from the main program.\n\nExternal links\n\nMPI_Abort man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"environment/#MPI.Init","page":"Environment","title":"MPI.Init","text":"Init()\n\nInitialize MPI in the current process.\n\nAll MPI programs must contain exactly one call to MPI.Init(). In particular, note that it is not valid to call MPI.Init again after calling MPI.Finalize.\n\nThe only MPI functions that may be called before MPI.Init() are MPI.Initialized and MPI.Finalized.\n\nExternal links\n\nMPI_Init man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"environment/#MPI.Initialized","page":"Environment","title":"MPI.Initialized","text":"Initialized()\n\nReturns true if MPI.Init has been called, false otherwise.\n\nIt is unaffected by MPI.Finalize, and is one of the few functions that may be called before MPI.Init.\n\nExternal links\n\nMPI_Intialized man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"environment/#MPI.Finalize","page":"Environment","title":"MPI.Finalize","text":"Finalize()\n\nMarks MPI state for cleanup. This should be called after Init, at most once, and no further MPI calls (other than Initialized or Finalized) should be made after it is called.\n\nNote that this does not correspond exactly to MPI_FINALIZE in the MPI specification. In particular:\n\nIt may not finalize MPI immediately. Julia will wait until all MPI-related objects are garbage collected before finalizing MPI. As a result, Finalized() may return false after Finalize() has been called. See Finalizers for more details.\nIt is optional: Init will automatically insert a hook to finalize MPI when Julia exits.\n\nExternal links\n\nMPI_Finalize man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"environment/#MPI.Finalized","page":"Environment","title":"MPI.Finalized","text":"Finalized()\n\nReturns true if MPI.Finalize has completed, false otherwise.\n\nIt is safe to call before MPI.Init and after MPI.Finalize.\n\nExternal links\n\nMPI_Finalized man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"environment/#MPI.refcount_inc","page":"Environment","title":"MPI.refcount_inc","text":"refcount_inc()\n\nIncrement the MPI reference counter. This should be called at initialization of any object which calls an MPI routine in its finalizer. A matching refcount_dec should be added to the finalizer.\n\nFor more details, see Finalizers.\n\n\n\n\n\n","category":"function"},{"location":"environment/#MPI.refcount_dec","page":"Environment","title":"MPI.refcount_dec","text":"refcount_dec()\n\nDecrement the MPI reference counter. This should be added after an MPI call in an object finalizer, with a matching refcount_inc when the object is initialized.\n\nFor more details, see Finalizers.\n\n\n\n\n\n","category":"function"},{"location":"comm/#Communicators-1","page":"Communicators","title":"Communicators","text":"","category":"section"},{"location":"comm/#","page":"Communicators","title":"Communicators","text":"An MPI communicator specifies the communication context for a communication operation. In particular, it specifies the set of processes which share the context, and assigns each each process a unique rank (see MPI.Comm_rank) taking an integer value in 0:n-1, where n is the number of processes in the communicator (see MPI.Comm_size.","category":"page"},{"location":"comm/#Types-1","page":"Communicators","title":"Types","text":"","category":"section"},{"location":"comm/#","page":"Communicators","title":"Communicators","text":"MPI.Comm","category":"page"},{"location":"comm/#MPI.Comm","page":"Communicators","title":"MPI.Comm","text":"MPI.Comm\n\nAn MPI Communicator object.\n\n\n\n\n\n","category":"type"},{"location":"comm/#Constants-1","page":"Communicators","title":"Constants","text":"","category":"section"},{"location":"comm/#","page":"Communicators","title":"Communicators","text":"MPI.COMM_WORLD\nMPI.COMM_SELF","category":"page"},{"location":"comm/#MPI.COMM_WORLD","page":"Communicators","title":"MPI.COMM_WORLD","text":"MPI.COMM_WORLD\n\nA communicator containing all processes with which the local rank can communicate at initialization. In a typical \"static-process\" model, this will be all processes.\n\n\n\n\n\n","category":"constant"},{"location":"comm/#MPI.COMM_SELF","page":"Communicators","title":"MPI.COMM_SELF","text":"MPI.COMM_SELF\n\nA communicator containing only the local process.\n\n\n\n\n\n","category":"constant"},{"location":"comm/#Functions-1","page":"Communicators","title":"Functions","text":"","category":"section"},{"location":"comm/#Accessors-1","page":"Communicators","title":"Accessors","text":"","category":"section"},{"location":"comm/#","page":"Communicators","title":"Communicators","text":"MPI.Comm_size\nMPI.Comm_rank","category":"page"},{"location":"comm/#MPI.Comm_size","page":"Communicators","title":"MPI.Comm_size","text":"Comm_size(comm::Comm)\n\nThe number of processes involved in communicator.\n\nSee also\n\nMPI.Comm_rank.\n\nExternal links\n\nMPI_Comm_size man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"comm/#MPI.Comm_rank","page":"Communicators","title":"MPI.Comm_rank","text":"Comm_rank(comm::Comm)\n\nThe rank of the process in the particular communicator's group.\n\nReturns an integer in the range 0:MPI.Comm_size()-1.\n\nSee also\n\nMPI.Comm_size.\n\nExternal links\n\nMPI_Comm_rank man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"comm/#Constructors-1","page":"Communicators","title":"Constructors","text":"","category":"section"},{"location":"comm/#","page":"Communicators","title":"Communicators","text":"MPI.Comm_dup\nMPI.Comm_get_parent\nMPI.Comm_spawn\nMPI.Comm_split\nMPI.Comm_split_type\nMPI.Intercomm_merge","category":"page"},{"location":"comm/#MPI.Comm_dup","page":"Communicators","title":"MPI.Comm_dup","text":"Comm_dup(comm::Comm)\n\nExternal links\n\nMPI_Comm_dup man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"comm/#MPI.Comm_get_parent","page":"Communicators","title":"MPI.Comm_get_parent","text":"Comm_get_parent()\n\nExternal links\n\nMPI_Comm_get_parent man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"comm/#MPI.Comm_spawn","page":"Communicators","title":"MPI.Comm_spawn","text":"Comm_spawn(command, argv::Vector{String}, nprocs::Integer, comm::Comm[, errors::Vector{Cint}]; kwargs...)\n\nExternal links\n\nMPI_Comm_spawn man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"comm/#MPI.Comm_split","page":"Communicators","title":"MPI.Comm_split","text":"Comm_split(comm::Comm, color::Integer, key::Integer)\n\nExternal links\n\nMPI_Comm_split man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"comm/#MPI.Comm_split_type","page":"Communicators","title":"MPI.Comm_split_type","text":"Comm_split_type(comm::Comm, split_type::Integer, key::Integer; kwargs...)\n\nExternal links\n\nMPI_Comm_split_type man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"comm/#MPI.Intercomm_merge","page":"Communicators","title":"MPI.Intercomm_merge","text":"Intercomm_merge(intercomm::Comm, flag::Bool)\n\nExternal links\n\nMPI_Intercomm_merge man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"comm/#Miscellaneous-1","page":"Communicators","title":"Miscellaneous","text":"","category":"section"},{"location":"comm/#","page":"Communicators","title":"Communicators","text":"MPI.universe_size","category":"page"},{"location":"examples/04-sendrecv/#Send/receive-1","page":"Send/receive","title":"Send/receive","text":"","category":"section"},{"location":"examples/04-sendrecv/#","page":"Send/receive","title":"Send/receive","text":"# examples/04-sendrecv.jl\nusing MPI\nMPI.Init()\n\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\nsize = MPI.Comm_size(comm)\n\ndst = mod(rank+1, size)\nsrc = mod(rank-1, size)\n\nN = 4\n\nsend_mesg = Array{Float64}(undef, N)\nrecv_mesg = Array{Float64}(undef, N)\n\nfill!(send_mesg, Float64(rank))\n\nrreq = MPI.Irecv!(recv_mesg, src,  src+32, comm)\n\nprint(\"$rank: Sending   $rank -> $dst = $send_mesg\\n\")\nsreq = MPI.Isend(send_mesg, dst, rank+32, comm)\n\nstats = MPI.Waitall!([rreq, sreq])\n\nprint(\"$rank: Received $src -> $rank = $recv_mesg\\n\")\n\nMPI.Barrier(comm)","category":"page"},{"location":"examples/04-sendrecv/#","page":"Send/receive","title":"Send/receive","text":"> mpiexec -n 3 julia examples/04-sendrecv.jl\n1: Sending   1 -> 2 = [1.0, 1.0, 1.0, 1.0]\n0: Sending   0 -> 1 = [0.0, 0.0, 0.0, 0.0]\n1: Received 0 -> 1 = [0.0, 0.0, 0.0, 0.0]\n2: Sending   2 -> 0 = [2.0, 2.0, 2.0, 2.0]\n0: Received 2 -> 0 = [2.0, 2.0, 2.0, 2.0]\n2: Received 1 -> 2 = [1.0, 1.0, 1.0, 1.0]","category":"page"},{"location":"installation/#Installation-1","page":"Installation","title":"Installation","text":"","category":"section"},{"location":"installation/#Requirements-1","page":"Installation","title":"Requirements","text":"","category":"section"},{"location":"installation/#Unix-systems-(Linux-and-MacOS)-1","page":"Installation","title":"Unix systems (Linux and MacOS)","text":"","category":"section"},{"location":"installation/#","page":"Installation","title":"Installation","text":"MPI.jl requires:","category":"page"},{"location":"installation/#","page":"Installation","title":"Installation","text":"A shared library MPI installation for C (supporting MPI standard 3.0), and\nA C compiler available via the mpicc command: this is required as part of the build process to determine the necessary type definitions and constants.","category":"page"},{"location":"installation/#","page":"Installation","title":"Installation","text":"This has been tested with:","category":"page"},{"location":"installation/#","page":"Installation","title":"Installation","text":"Open MPI\nMPICH\nIntel MPI","category":"page"},{"location":"installation/#Windows-1","page":"Installation","title":"Windows","text":"","category":"section"},{"location":"installation/#","page":"Installation","title":"Installation","text":"MPI.jl requires the Microsoft MPI (MS-MPI) runtime to be installed.","category":"page"},{"location":"installation/#Building-1","page":"Installation","title":"Building","text":"","category":"section"},{"location":"installation/#","page":"Installation","title":"Installation","text":"The MPI.jl package can be installed via add MPI in the Julia package manager. ","category":"page"},{"location":"installation/#","page":"Installation","title":"Installation","text":"The build script will attempt to find the shared library and constants: this can be controlled with the optional environment variables:","category":"page"},{"location":"installation/#","page":"Installation","title":"Installation","text":"JULIA_MPI_PATH: the top-level installation directory of MPI.\nJULIA_MPI_LIBRARY: the path of the MPI shared library.\nJULIA_MPI_LIBRARY_PATH: the directory containing the MPI library files.\nJULIA_MPI_INCLUDE_PATH: the directory containing the MPI header files.\nJULIA_MPI_CFLAGS: C flags passed to the constant generation build (default: -lmpi)\nJULIA_MPICC: MPI C compiler (default: mpicc)\nJULIA_MPIEXEC: MPI startup command (default: mpiexec)","category":"page"},{"location":"installation/#","page":"Installation","title":"Installation","text":"If your MPI installation changes (e.g. it is upgraded by the system, or you switch libraries), you will need to re-run build MPI at the package prompt.","category":"page"},{"location":"functions/#Function-reference-1","page":"Function reference","title":"Function reference","text":"","category":"section"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"The following functions are currently wrapped, with the convention: MPI_Fun => MPI.Fun","category":"page"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"Constants like MPI_SUM are wrapped as MPI.SUM.   Note also that arbitrary Julia functions f(x,y) can be passed as reduction operations to the MPI Allreduce and Reduce functions.","category":"page"},{"location":"functions/#Datatype-functions-1","page":"Function reference","title":"Datatype functions","text":"","category":"section"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"Julia Function (assuming import MPI) C Function\nMPI.Get_address MPI_Get_address\nMPI.mpitype MPI_Type_create_struct/MPI_Type_commit","category":"page"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"note: Note\nmpitype is not strictly a wrapper for MPI_Type_create_struct and MPI_Type_commit, it also is an accessor for previously created types.","category":"page"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"MPI.Get_address\nMPI.mpitype","category":"page"},{"location":"functions/#One-sided-communication-1","page":"Function reference","title":"One-sided communication","text":"","category":"section"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"Julia Function (assuming import MPI) C Function\nMPI.Win_create MPI_Win_create\nMPI.Win_create_dynamic MPI_Win_create_dynamic\nMPI.Win_allocate_shared MPI_Win_allocate_shared\nMPI.Win_shared_query MPI_Win_shared_query\nMPI.Win_attach MPI_Win_attach\nMPI.Win_detach MPI_Win_detach\nMPI.Win_fence MPI_Win_fence\nMPI.Win_flush MPI_Win_flush\nMPI.Win_free MPI_Win_free\nMPI.Win_sync MPI_Win_sync\nMPI.Win_lock MPI_Win_lock\nMPI.Win_unlock MPI_Win_unlock\nMPI.Get MPI_Get\nMPI.Put MPI_Put\nMPI.Fetch_and_op MPI_Fetch_and_op\nMPI.Accumulate MPI_Accumulate\nMPI.Get_accumulate MPI_Get_accumulate","category":"page"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"MPI.Win_create\nMPI.Win_create_dynamic\nMPI.Win_allocate_shared\nMPI.Win_shared_query\nMPI.Win_attach\nMPI.Win_detach\nMPI.Win_fence\nMPI.Win_flush\nMPI.Win_free\nMPI.Win_sync\nMPI.Win_lock\nMPI.Win_unlock\nMPI.Get\nMPI.Put\nMPI.Fetch_and_op\nMPI.Accumulate\nMPI.Get_accumulate","category":"page"},{"location":"functions/#MPI.Win_create","page":"Function reference","title":"MPI.Win_create","text":"MPI.Win_create(base::Array, comm::Comm; infokws...)\n\nCreate a window over the array base, returning a Win object used by these processes to perform RMA operations\n\nThis is a collective call over comm.\n\ninfokws are info keys providing optimization hints.\n\nMPI.free should be called on the Win object once operations have been completed.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Win_create_dynamic","page":"Function reference","title":"MPI.Win_create_dynamic","text":"MPI.Win_create_dynamic(comm::Comm; infokws...)\n\nCreate a dynamic window returning a Win object used by these processes to perform RMA operations\n\nThis is a collective call over comm.\n\ninfokws are info keys providing optimization hints.\n\nMPI.free should be called on the Win object once operations have been completed.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Win_allocate_shared","page":"Function reference","title":"MPI.Win_allocate_shared","text":"(win, ptr) = MPI.Win_allocate_shared(T, len, comm::Comm; infokws...)\n\nCreate and allocate a shared memory window for objects of type T of length len, returning a Win and a Ptr{T} object used by these processes to perform RMA operations\n\nThis is a collective call over comm.\n\ninfokws are info keys providing optimization hints.\n\nMPI.free should be called on the Win object once operations have been completed.\n\n\n\n\n\n","category":"function"},{"location":"functions/#Info-objects-1","page":"Function reference","title":"Info objects","text":"","category":"section"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"MPI.Info\nMPI.infoval","category":"page"},{"location":"functions/#MPI.Info","page":"Function reference","title":"MPI.Info","text":"Info <: AbstractDict{Symbol,String}\n\nMPI.Info objects store key-value pairs, and are typically used for passing optional arguments to MPI functions.\n\nUsage\n\nThese will typically be hidden from user-facing APIs by splatting keywords, e.g.\n\nfunction f(args...; kwargs...)\n    info = Info(kwargs...)\n    # pass `info` object to `ccall`\nend\n\nFor manual usage, Info objects act like Julia Dict objects:\n\ninfo = Info(init=true) # keyword argument is required\ninfo[key] = value\nx = info[key]\ndelete!(info, key)\n\nIf init=false is used in the costructor (the default), a \"null\" Info object will be returned: no keys can be added to such an object.\n\n\n\n\n\n","category":"type"},{"location":"functions/#MPI.infoval","page":"Function reference","title":"MPI.infoval","text":"infoval(x)\n\nConvert Julia object x to a string representation for storing in an Info object.\n\nThe MPI specification allows passing strings, Boolean values, integers, and lists.\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#Point-to-point-communication-1","page":"Point-to-point communication","title":"Point-to-point communication","text":"","category":"section"},{"location":"pointtopoint/#Types-1","page":"Point-to-point communication","title":"Types","text":"","category":"section"},{"location":"pointtopoint/#","page":"Point-to-point communication","title":"Point-to-point communication","text":"MPI.Request\nMPI.Status","category":"page"},{"location":"pointtopoint/#MPI.Request","page":"Point-to-point communication","title":"MPI.Request","text":"MPI.Request\n\nAn MPI Request object, representing a non-blocking communication. This also contains a reference to the buffer used in the communication to ensure it isn't garbage-collected during communication.\n\nThe status of a Request can be checked by the Wait! and Test! functions or their multiple-request variants, which will deallocate the request once it is determined to be complete. Alternatively, it will be deallocated at finalization, meaning that it is safe to ignore the request objects if the status of the communication can be checked by other means.\n\nSee also Cancel!.\n\n\n\n\n\n","category":"type"},{"location":"pointtopoint/#MPI.Status","page":"Point-to-point communication","title":"MPI.Status","text":"MPI.Status\n\nThe status of an MPI receive communication. It has 3 accessible fields\n\nsource: source of the received message\ntag: tag of the received message\nerror: error code. This is only set if a function returns multiple statuses.\n\nAdditionally, the accessor function MPI.Get_count can be used to determine the number of entries received.\n\n\n\n\n\n","category":"type"},{"location":"pointtopoint/#Functions-1","page":"Point-to-point communication","title":"Functions","text":"","category":"section"},{"location":"pointtopoint/#Accessor-1","page":"Point-to-point communication","title":"Accessor","text":"","category":"section"},{"location":"pointtopoint/#","page":"Point-to-point communication","title":"Point-to-point communication","text":"MPI.Get_count","category":"page"},{"location":"pointtopoint/#MPI.Get_count","page":"Point-to-point communication","title":"MPI.Get_count","text":"MPI.Get_count(status::Status, T)\n\nThe number of entries received. T should match the argument provided by the receive call that set the status variable.\n\nIf the number of entries received exceeds the limits of the count parameter, then it returns MPI_UNDEFINED.\n\nExternal links\n\nMPI_Get_count man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#Blocking-1","page":"Point-to-point communication","title":"Blocking","text":"","category":"section"},{"location":"pointtopoint/#","page":"Point-to-point communication","title":"Point-to-point communication","text":"MPI.Send\nMPI.send\nMPI.Recv!\nMPI.Recv\nMPI.recv\nMPI.Sendrecv!","category":"page"},{"location":"pointtopoint/#MPI.Send","page":"Point-to-point communication","title":"MPI.Send","text":"Send(buf, [count::Integer, [datatype::Datatype,]]\n     dest::Integer, tag::Integer, comm::Comm) where T\n\nPerform a blocking send of count elements of type datatype from buf to MPI rank dest of communicator comm using the message tag tag.\n\nIf not provided, datatype and count are derived from the element type and length of buf, respectively.\n\nExternal links\n\nMPI_Send man page: OpenMPI, MPICH\n\n\n\n\n\nSend(obj::T, dest::Integer, tag::Integer, comm::Comm) where T\n\nComplete a blocking send of obj to MPI rank dest of communicator comm using with the message tag tag.\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.send","page":"Point-to-point communication","title":"MPI.send","text":"send(obj, dest::Integer, tag::Integer, comm::Comm)\n\nComplete a blocking send using a serialized version of obj to MPI rank dest of communicator comm using with the message tag tag.\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Recv!","page":"Point-to-point communication","title":"MPI.Recv!","text":"Recv!(buf, [count::Integer, [datatype::Datatype,]]\n      src::Integer, tag::Integer, comm::Comm)\n\nCompletes a blocking receive of up to count elements of type datatype into buf from MPI rank src of communicator comm using with the message tag tag.\n\nIf not provided, datatype and count are derived from the element type and length of buf, respectively.\n\nReturns the Status of the receive.\n\nExternal links\n\nMPI_Recv man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Recv","page":"Point-to-point communication","title":"MPI.Recv","text":"Recv(::Type{T}, src::Integer, tag::Integer, comm::Comm)\n\nCompletes a blocking receive of a buffer of type T from MPI rank src of communicator comm using with the message tag tag.\n\nReturns the buffer of type T and the Status of the receive.\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.recv","page":"Point-to-point communication","title":"MPI.recv","text":"recv(src::Integer, tag::Integer, comm::Comm)\n\nCompletes a blocking receive of a serialized object from MPI rank src of communicator comm using with the message tag tag.\n\nReturns the deserialized object and the Status of the receive.\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Sendrecv!","page":"Point-to-point communication","title":"MPI.Sendrecv!","text":"Sendrecv!(sendbuf, [sendcount::Integer, [sendtype::Union{Datatype, MPI_Datatype}]],\n         dest::Integer, sendtag::Integer,\n         recvbuf, [recvcount::Integer, [recvtype::Union{Datatype, MPI_Datatype}]],\n         source::Integer, recvtag::Integer,\n         comm::Comm)\n\nComplete a blocking send-receive operation over the MPI communicator comm. Send sendcount elements of type sendtype from sendbuf to the MPI rank dest using message tag tag, and receive recvcount elements of type recvtype from MPI rank source into the buffer recvbuf using message tag tag. Return a Status object.\n\nIf not provided, sendtype/recvtype and sendcount/recvcount are derived from the element type and length of sendbuf/recvbuf, respectively.\n\nExternal links\n\nMPI_Sendrecv man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#Non-blocking-1","page":"Point-to-point communication","title":"Non-blocking","text":"","category":"section"},{"location":"pointtopoint/#","page":"Point-to-point communication","title":"Point-to-point communication","text":"MPI.Isend\nMPI.isend\nMPI.Irecv!\nMPI.Test!\nMPI.Testall!\nMPI.Testany!\nMPI.Testsome!\nMPI.Wait!\nMPI.Waitall!\nMPI.Waitany!\nMPI.Waitsome!","category":"page"},{"location":"pointtopoint/#MPI.Isend","page":"Point-to-point communication","title":"MPI.Isend","text":"Isend(buf, [count::Integer, [datatype::Datatype,]]\n      dest::Integer, tag::Integer, comm::Comm)\n\nStarts a nonblocking send of count elements of type datatype from buf to MPI rank dest of communicator comm using with the message tag tag.\n\nIf not provided, datatype and count are derived from the element type and length of buf, respectively.\n\nReturns the Request object for the nonblocking send.\n\nExternal links\n\nMPI_Isend man page: OpenMPI, MPICH\n\n\n\n\n\nIsend(obj::T, dest::Integer, tag::Integer, comm::Comm) where T\n\nStarts a nonblocking send of obj to MPI rank dest of communicator comm using with the message tag tag.\n\nReturns the commication Request for the nonblocking send.\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.isend","page":"Point-to-point communication","title":"MPI.isend","text":"isend(obj, dest::Integer, tag::Integer, comm::Comm)\n\nStarts a nonblocking send of using a serialized version of obj to MPI rank dest of communicator comm using with the message tag tag.\n\nReturns the commication Request for the nonblocking send.\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Irecv!","page":"Point-to-point communication","title":"MPI.Irecv!","text":"Irecv!(buf, [count::Integer, [datatype::Datatype,]]\n       src::Integer, tag::Integer, comm::Comm) where T\n\nStarts a nonblocking receive of up to count elements of type datatype into buf from MPI rank src of communicator comm using with the message tag tag\n\nReturns the Request for the nonblocking receive.\n\nExternal links\n\nMPI_Irecv man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Test!","page":"Point-to-point communication","title":"MPI.Test!","text":"(flag, status) = Test!(req::Request)\n\nCheck if the request req is complete. If so, the request is deallocated and flag is returned true and status as the Status of the request. Otherwise flag is returned false and status is nothing.\n\nExternal links\n\nMPI_Test man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Testall!","page":"Point-to-point communication","title":"MPI.Testall!","text":"(flag, statuses) = Testall!(reqs::Vector{Request})\n\nCheck if all active requests in the array reqs are complete. If so, the requests are deallocated and flag is returned as true and statuses is an array of the Status objects corresponding to each request is returned. Otherwise no requests are modified a tuple of false, nothing is returned.\n\nExternal links\n\nMPI_Testall man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Testany!","page":"Point-to-point communication","title":"MPI.Testany!","text":"(flag, index, status) = Testany!(reqs::Vector{Request})\n\nCheck if any one of the requests in the array reqs is complete.\n\nIf one or more requests are complete, then one is chosen arbitrarily, deallocated and flag is returned as true, along with the index and the Status of the request.\n\nOtherwise, if there are no complete requests, then index is returned as 0, status as nothing, and flag as true if there are no active requests and false otherwise.\n\nExternal links\n\nMPI_Testany man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Testsome!","page":"Point-to-point communication","title":"MPI.Testsome!","text":"(indices, statuses) = Testsome!(reqs::Vector{Request})\n\nSimilar to Waitsome! except that it returns immediately: if no operations have completed then indices and statuses will be empty.\n\nExternal links\n\nMPI_Testsome man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Wait!","page":"Point-to-point communication","title":"MPI.Wait!","text":"status = Wait!(req::Request)\n\nBlock until the request req is complete and deallocated. Returns the Status of the request.\n\nExternal links\n\nMPI_Wait man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Waitall!","page":"Point-to-point communication","title":"MPI.Waitall!","text":"statuses = Waitall!(reqs::Vector{Request})\n\nBlock until all active requests in the array reqs are complete. Returns an array of the Status objects corresponding to each request.\n\nExternal links\n\nMPI_Waitall man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Waitany!","page":"Point-to-point communication","title":"MPI.Waitany!","text":"(index, status) = Waitany!(reqs::Vector{Request})\n\nBlocks until one of the requests in the array reqs is complete: if more than one is complete, one is chosen arbitrarily. The request is deallocated and a tuple of the index of the completed request and its Status is returned. If there are no active requests, then index is returned as 0.\n\nExternal links\n\nMPI_Waitany man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Waitsome!","page":"Point-to-point communication","title":"MPI.Waitsome!","text":"(indices, statuses) = Waitsome!(reqs::Vector{Request})\n\nBlock until at least one of the active requests in the array reqs is complete. The completed requests are deallocated, and a tuple of their indices in reqs and their corresponding Status objects are returned. If there are no active requests, then the function returns immediately and indices and statuses are empty.\n\nExternal links\n\nMPI_Waitsome man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#Probe-and-cancel-1","page":"Point-to-point communication","title":"Probe and cancel","text":"","category":"section"},{"location":"pointtopoint/#","page":"Point-to-point communication","title":"Point-to-point communication","text":"MPI.Iprobe\nMPI.Probe\nMPI.Cancel!","category":"page"},{"location":"pointtopoint/#MPI.Iprobe","page":"Point-to-point communication","title":"MPI.Iprobe","text":"ismessage, (status|nothing) = Iprobe(src::Integer, tag::Integer, comm::Comm)\n\nChecks if there is a message that can be received matching src, tag and comm. If so, returns a tuple true and a Status object, otherwise returns a tuple false, nothing.\n\nExternal links\n\nMPI_Iprobe man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Probe","page":"Point-to-point communication","title":"MPI.Probe","text":"status = Probe(src::Integer, tag::Integer, comm::Comm)\n\nBlocks until there is a message that can be received matching src, tag and comm. Returns the corresponding Status object.\n\nExternal links\n\nMPI_Probe man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Cancel!","page":"Point-to-point communication","title":"MPI.Cancel!","text":"Cancel!(req::Request)\n\nMarks a pending Irecv! operation for cancellation (cancelling a Isend, while supported in some implementations, is deprecated as of MPI 3.1). Note that the request is not deallocated, and can still be queried using the test or wait functions.\n\nExternal links\n\nMPI_Cancel man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"examples/02-broadcast/#Broadcast-1","page":"Broadcast","title":"Broadcast","text":"","category":"section"},{"location":"examples/02-broadcast/#","page":"Broadcast","title":"Broadcast","text":"# examples/02-broadcast.jl\nimport MPI\nMPI.Init()\n\ncomm = MPI.COMM_WORLD\nN = 5\nroot = 0\n\nif MPI.Comm_rank(comm) == root\n    print(\" Running on $(MPI.Comm_size(comm)) processes\\n\")\nend\nMPI.Barrier(comm)\n\nif MPI.Comm_rank(comm) == root\n    A = [i*(1.0 + im*2.0) for i = 1:N]\nelse\n    A = Array{ComplexF64}(undef, N)\nend\n\nMPI.Bcast!(A, root, comm)\n\nprint(\"rank = $(MPI.Comm_rank(comm)), A = $A\\n\")\n\nif MPI.Comm_rank(comm) == root\n    B = Dict(\"foo\" => \"bar\")\nelse\n    B = nothing\nend\n\nB = MPI.bcast(B, root, comm)\nprint(\"rank = $(MPI.Comm_rank(comm)), B = $B\\n\")\n\nif MPI.Comm_rank(comm) == root\n    f = x -> x^2 + 2x - 1\nelse\n    f = nothing\nend\n\nf = MPI.bcast(f, root, comm)\nprint(\"rank = $(MPI.Comm_rank(comm)), f(3) = $(f(3))\\n\")","category":"page"},{"location":"examples/02-broadcast/#","page":"Broadcast","title":"Broadcast","text":"> mpiexec -n 3 julia examples/02-broadcast.jl\n Running on 3 processes\nrank = 1, A = Complex{Float64}[1.0+2.0im, 2.0+4.0im, 3.0+6.0im, 4.0+8.0im, 5.0+10.0im]\nrank = 2, A = Complex{Float64}[1.0+2.0im, 2.0+4.0im, 3.0+6.0im, 4.0+8.0im, 5.0+10.0im]\nrank = 0, A = Complex{Float64}[1.0+2.0im, 2.0+4.0im, 3.0+6.0im, 4.0+8.0im, 5.0+10.0im]\nrank = 0, B = Dict(\"foo\"=>\"bar\")\nrank = 2, B = Dict(\"foo\"=>\"bar\")\nrank = 1, B = Dict(\"foo\"=>\"bar\")\nrank = 0, f(3) = 14\nrank = 2, f(3) = 14\nrank = 1, f(3) = 14","category":"page"},{"location":"examples/03-reduce/#Reduce-1","page":"Reduce","title":"Reduce","text":"","category":"section"},{"location":"examples/03-reduce/#","page":"Reduce","title":"Reduce","text":"# examples/03-reduce.jl\nusing MPI\nMPI.Init()\n\ncomm = MPI.COMM_WORLD\nroot = 0\n\nr = MPI.Comm_rank(comm)\n\nsr = MPI.Reduce(r, +, root, comm)\n\nif MPI.Comm_rank(comm) == root\n    println(\"sum of ranks = $sr\")\nend\n","category":"page"},{"location":"examples/03-reduce/#","page":"Reduce","title":"Reduce","text":"> mpiexec -n 3 julia examples/03-reduce.jl\nsum of ranks = 3","category":"page"},{"location":"usage/#Usage-1","page":"Usage","title":"Usage","text":"","category":"section"},{"location":"usage/#","page":"Usage","title":"Usage","text":"MPI is based on a single program, multiple data (SPMD) model, where multiple processes are launched running independent programs, which then communicate as necessary via messages.","category":"page"},{"location":"usage/#Basic-example-1","page":"Usage","title":"Basic example","text":"","category":"section"},{"location":"usage/#","page":"Usage","title":"Usage","text":"A script should include using MPI and MPI.Init() statements before calling any MPI operaions, for example","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"# examples/01-hello.jl\nusing MPI\nMPI.Init()\n\ncomm = MPI.COMM_WORLD\nprintln(\"Hello world, I am $(MPI.Comm_rank(comm)) of $(MPI.Comm_size(comm))\")\nMPI.Barrier(comm)","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"Calling MPI.Finalize() at the end of the program is optional, as it will be called automatically when Julia exits.","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"The program can then be launched via an MPI launch command (typically mpiexec, mpirun or srun), e.g.","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"$ mpiexec -n 3 julia --project examples/01-hello.jl\nHello world, I am rank 0 of 3\nHello world, I am rank 2 of 3\nHello world, I am rank 1 of 3","category":"page"},{"location":"usage/#CUDA-aware-MPI-support-1","page":"Usage","title":"CUDA-aware MPI support","text":"","category":"section"},{"location":"usage/#","page":"Usage","title":"Usage","text":"If your MPI implementation has been compiled with CUDA support, then CuArrays (from the CuArrays.jl package) can be passed directly as send and receive buffers for point-to-point and collective operations (they may also work with one-sided operations, but these are not often supported).","category":"page"},{"location":"usage/#Finalizers-1","page":"Usage","title":"Finalizers","text":"","category":"section"},{"location":"usage/#","page":"Usage","title":"Usage","text":"In order to ensure MPI routines are called in the correct order at finalization time, MPI.jl maintains a reference count. If you define an object that needs to call an MPI routine during its finalization, you should call MPI.refcount_inc() when it is initialized, and MPI.refcount_dec() in its finalizer (after the relevant MPI call).","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"For example","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"mutable struct MyObject\n    ...\n    function MyObject(args...)\n        obj = new(args...)\n        # MPI call to create object\n        refcount_inc()\n        finalizer(obj) do x\n            # MPI call to free object\n            refcount_dec()\n        end\n        return obj\n    end\nend","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"","category":"page"},{"location":"collective/#Collective-communication-1","page":"Collective communication","title":"Collective communication","text":"","category":"section"},{"location":"collective/#Synchronization-1","page":"Collective communication","title":"Synchronization","text":"","category":"section"},{"location":"collective/#","page":"Collective communication","title":"Collective communication","text":"MPI.Barrier","category":"page"},{"location":"collective/#MPI.Barrier","page":"Collective communication","title":"MPI.Barrier","text":"Barrier(comm::Comm)\n\nBlocks until comm is synchronized.\n\nIf comm is an intracommunicator, then it blocks until all members of the group have called it.\n\nIf comm is an intercommunicator, then it blocks until all members of the other group have called it.\n\nExternal links\n\nMPI_Barrier man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#Broadcast-1","page":"Collective communication","title":"Broadcast","text":"","category":"section"},{"location":"collective/#","page":"Collective communication","title":"Collective communication","text":"MPI.Bcast!","category":"page"},{"location":"collective/#MPI.Bcast!","page":"Collective communication","title":"MPI.Bcast!","text":"Bcast!(buf[, count=length(buf)], root::Integer, comm::Comm)\n\nBroadcast the first count elements of the buffer buf from root to all processes.\n\nExternal links\n\nMPI_Bcast man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#Gather/Scatter-1","page":"Collective communication","title":"Gather/Scatter","text":"","category":"section"},{"location":"collective/#Gather-1","page":"Collective communication","title":"Gather","text":"","category":"section"},{"location":"collective/#","page":"Collective communication","title":"Collective communication","text":"MPI.Allgather!\nMPI.Allgather\nMPI.Allgatherv!\nMPI.Allgatherv\nMPI.Gather!\nMPI.Gather\nMPI.Gatherv!\nMPI.Gatherv","category":"page"},{"location":"collective/#MPI.Allgather!","page":"Collective communication","title":"MPI.Allgather!","text":"Allgather!(sendbuf, recvbuf[, count::Integer=length(sendbuf)], comm::Comm)\nAllgather!(sendrecvbuf, count::Integer, comm::Comm)\n\nEach process sends the first count elements of sendbuf to the other processes, who store the results in rank order into recvbuf.\n\nIf only one buffer sendrecvbuf is provided, then each process send data is assumed to be in the area where it would receive it's own contribution.\n\nSee also\n\nAllgather for the allocating operation\nAllgatherv!/Allgatherv if the number of elements varies between processes.\nGather! to send only to a single root process\n\nExternal links\n\nMPI_Allgather man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Allgather","page":"Collective communication","title":"MPI.Allgather","text":"Allgather(sendbuf[, count=length(sendbuf)], comm)\n\nEach process sends the first count elements of sendbuf to the other processes, who store the results in rank order allocating the output buffer.\n\nSee also\n\nAllgather! for the mutating operation\nAllgatherv!/Allgatherv if the number of elements varies between processes.\nGather! to send only to a single root process\n\nExternal links\n\nMPI_Allgather man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Allgatherv!","page":"Collective communication","title":"MPI.Allgatherv!","text":"Allgatherv!(sendbuf, recvbuf, counts, comm)\nAllgatherv!(sendrecvbuf, counts, comm)\n\nEach process sends the first counts[rank] elements of the buffer sendbuf to all other process. Each process stores the received data in rank order in the buffer recvbuf.\n\nIf only one buffer sendrecvbuf is provided, then for each process, the data to be sent is taken from the interval of recvbuf where it would store it's own data.\n\nSee also\n\nAllgatherv for the allocating operation\nGatherv!/Gatherv to send the result to a single process\n\nExternal links\n\nMPI_Allgatherv man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Allgatherv","page":"Collective communication","title":"MPI.Allgatherv","text":"Allgatherv(sendbuf, counts, comm)\n\nEach process sends the first counts[rank] elements of the buffer sendbuf to all other process. Each process allocates an output buffer and stores the received data in rank order.\n\nSee also\n\nAllgatherv! for the mutating operation.\nGatherv!/Gatherv to send the result to a single process.\n\nExternal links\n\nMPI_Allgatherv man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Gather!","page":"Collective communication","title":"MPI.Gather!","text":"Gather!(sendbuf, recvbuf[, count::Integer=length(sendbuf)], root::Integer, comm::Comm)\n\nEach process sends the first count elements of the buffer sendbuf to the root process. The root process stores elements in rank order in the buffer buffer recvbuf.\n\nsendbuf can be nothing on the root process, in which case the corresponding entries in recvbuf are assumed to be already in place (this corresponds the behaviour of MPI_IN_PLACE in MPI_Gather). For example\n\nif root == MPI.Comm_rank(comm)\n    Gather!(nothing, buf, count, root, comm)\nelse\n    Gather!(buf, nothing, count, root, comm)\nend\n\nrecvbuf on the root process should be a buffer of length count*Comm_size(comm), and on non-root processes it is ignored and can be nothing.\n\ncount should be the same for all processes.\n\nSee also\n\nGather for the allocating operation.\nGatherv! if the number of elements varies between processes.\nAllgather! to send the result to all processes.\n\nExternal links\n\nMPI_Gather man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Gather","page":"Collective communication","title":"MPI.Gather","text":"Gather(sendbuf[, count=length(sendbuf)], root, comm)\n\nEach process sends the first count elements of the buffer sendbuf to the root process. The root allocates the output buffer and stores elements in rank order.\n\nSee also\n\nGather! for the mutating operation.\nGatherv!/Gatherv if the number of elements varies between processes.\nAllgather!/Allgather to send the result to all processes.\n\nExternal links\n\nMPI_Gather man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Gatherv!","page":"Collective communication","title":"MPI.Gatherv!","text":"Gatherv!(sendbuf, recvbuf, counts, root, comm)\n\nEach process sends the first counts[rank] elements of the buffer sendbuf to the root process. The root stores elements in rank order in the buffer recvbuf.\n\nsendbuf can be nothing on the root process, in which case the corresponding entries in recvbuf are assumed to be already in place (this corresponds the behaviour of MPI_IN_PLACE in MPI_Gatherv). For example\n\nif root == MPI.Comm_rank(comm)\n    Gatherv!(nothing, buf, counts, root, comm)\nelse\n    Gatherv!(buf, nothing, counts, root, comm)\nend\n\nSee also\n\nGatherv for the allocating operation\nGather!\nAllgatherv!/Allgatherv to send the result to all processes\n\nExternal links\n\nMPI_Gatherv man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Gatherv","page":"Collective communication","title":"MPI.Gatherv","text":"Gatherv(sendbuf, counts, root, comm)\n\nEach process sends the first counts[rank] elements of the buffer sendbuf to the root process. The root allocates the output buffer and stores elements in rank order.\n\nSee also\n\nGatherv! for the mutating operation\nGather!/Gather\nAllgatherv!/Allgatherv to send the result to all processes\n\nExternal links\n\nMPI_Gatherv man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#Scatter-1","page":"Collective communication","title":"Scatter","text":"","category":"section"},{"location":"collective/#","page":"Collective communication","title":"Collective communication","text":"MPI.Scatter!\nMPI.Scatter\nMPI.Scatterv!\nMPI.Scatterv","category":"page"},{"location":"collective/#MPI.Scatter!","page":"Collective communication","title":"MPI.Scatter!","text":"Scatter!(sendbuf, recvbuf[, count=length(recvbuf)], root::Integer, comm::Comm)\n\nSplits the buffer sendbuf in the root process into Comm_size(comm) chunks of length count, and sends the j-th chunk to the process of rank j into the recvbuf buffer.\n\nsendbuf on the root process should be a buffer of length count*Comm_size(comm), and on non-root processes it is ignored and can be nothing.\n\nrecvbuf can be nothing on the root process, in which case it is unmodified (this corresponds the behaviour of MPI_IN_PLACE in MPI_Scatter). For example\n\nif root == MPI.Comm_rank(comm)\n    Scatter!(buf, nothing, count, root, comm)\nelse\n    Scatter!(nothing, buf, count, root, comm)        \nend\n\ncount should be the same for all processes.\n\nSee also\n\nScatter to allocate the output buffer.\nScatterv! if the number of elements varies between processes.\n\nExternal links\n\nMPI_Scatter man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Scatter","page":"Collective communication","title":"MPI.Scatter","text":"Scatter(sendbuf, count, root, comm)\n\nSplits the buffer sendbuf in the root process into Comm_size(comm) chunks and sends the j-th chunk to the process of rank j, allocating the output buffer.\n\nSee also\n\nScatter! for the mutating operation.\nScatterv! if the number of elements varies between processes.\n\nExternal links\n\nMPI_Scatter man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Scatterv!","page":"Collective communication","title":"MPI.Scatterv!","text":"Scatterv!(sendbuf, recvbuf, counts, root, comm)\n\nSplits the buffer sendbuf in the root process into Comm_size(comm) chunks of length counts[j] and sends the j-th chunk to the process of rank j into the recvbuf buffer, which must be of length at least count.\n\nrecvbuf can be nothing on the root process, in which case it is unmodified (this corresponds the behaviour of MPI_IN_PLACE in MPI_Scatterv). For example\n\nif root == MPI.Comm_rank(comm)\n    Scatterv!(buf, nothing, counts, root, comm)\nelse\n    Scatterv!(nothing, buf, counts, root, comm)\nend\n\nSee also\n\nScatterv for the allocating operation\nScatter!/Scatter if the counts are the same for all processes\n\nExternal links\n\nMPI_Scatterv man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Scatterv","page":"Collective communication","title":"MPI.Scatterv","text":"Scatterv(sendbuf, counts, root, comm)\n\nSplits the buffer sendbuf in the root process into Comm_size(comm) chunks of length counts[j] and sends the j-th chunk to the process of rank j, which allocates the output buffer\n\nSee also\n\nScatterv! for the mutating operation\nScatter!/Scatter if the counts are the same for all processes\n\nExternal links\n\nMPI_Scatterv man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#All-to-all-1","page":"Collective communication","title":"All-to-all","text":"","category":"section"},{"location":"collective/#","page":"Collective communication","title":"Collective communication","text":"MPI.Alltoall!\nMPI.Alltoall\nMPI.Alltoallv!\nMPI.Alltoallv","category":"page"},{"location":"collective/#MPI.Alltoall!","page":"Collective communication","title":"MPI.Alltoall!","text":"Alltoall!(sendbuf, recvbuf, count::Integer, comm::Comm)\nAlltoall!(sendrecvbuf, count::Integer, comm::Comm)\n\nEvery process divides the buffer sendbuf into Comm_size(comm) chunks of length count, sending the j-th chunk to the j-th process. Every process stores the data received from the j-th process in the j-th chunk of the buffer recvbuf.\n\nrank    send buf                        recv buf\n----    --------                        --------\n 0      a,b,c,d,e,f       Alltoall      a,b,A,B,α,β\n 1      A,B,C,D,E,F  ---------------->  c,d,C,D,γ,ψ\n 2      α,β,γ,ψ,η,ν                     e,f,E,F,η,ν\n\nIf only one buffer sendrecvbuf then data is overwritten.\n\nSee also\n\nAlltoall for the allocating operation\n\nExternal links\n\nMPI_Alltoall man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Alltoall","page":"Collective communication","title":"MPI.Alltoall","text":"Alltoall(sendbuf, count::Integer, comm::Comm)\n\nEvery process divides the buffer sendbuf into Comm_size(comm) chunks of length count, sending the j-th chunk to the j-th process. Every process allocates the output buffer and stores the data received from the j-th process in the j-th chunk.\n\nrank    send buf                        recv buf\n----    --------                        --------\n 0      a,b,c,d,e,f       Alltoall      a,b,A,B,α,β\n 1      A,B,C,D,E,F  ---------------->  c,d,C,D,γ,ψ\n 2      α,β,γ,ψ,η,ν                     e,f,E,F,η,ν\n\nSee also\n\nAlltoall! for the mutating operation\n\nExternal links\n\nMPI_Alltoall man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Alltoallv!","page":"Collective communication","title":"MPI.Alltoallv!","text":"Alltoallv!(sendbuf, recvbuf, scounts::Vector, rcounts::Vector, comm::Comm)\n\nSimilar to Alltoall!, except with different size chunks per process.\n\nSee also\n\nAlltoallv for the allocating operation\n\nExternal links\n\nMPI_Alltoallv man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Alltoallv","page":"Collective communication","title":"MPI.Alltoallv","text":"Alltoallv(sendbuf, recvbuf, scounts::Vector, rcounts::Vector, comm::Comm)\n\nSimilar to Alltoall, except with different size chunks per process.\n\nSee also\n\nAlltoallv! for the mutating operation\n\nExternal links\n\nMPI_Alltoallv man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#Reduce/Scan-1","page":"Collective communication","title":"Reduce/Scan","text":"","category":"section"},{"location":"collective/#","page":"Collective communication","title":"Collective communication","text":"MPI.Reduce!\nMPI.Reduce\nMPI.Allreduce!\nMPI.Allreduce\nMPI.Scan!\nMPI.Scan\nMPI.Exscan!\nMPI.Exscan","category":"page"},{"location":"collective/#MPI.Reduce!","page":"Collective communication","title":"MPI.Reduce!","text":"Reduce!(sendbuf, recvbuf[, count::Integer=length(sendbuf)], op, root::Integer, comm::Comm)\nReduce!(sendrecvbuf, op, root::Integer, comm::Comm)\n\nPerforms elementwise reduction using the operator op on the first count elements of the buffer sendbuf and stores the result in recvbuf on the process of rank root.\n\nOn non-root processes recvbuf is ignored, and can be nothing. \n\nTo perform the reduction in place, provide a single buffer sendrecvbuf.\n\nSee also\n\nReduce to handle allocation of the output buffer.\nAllreduce!/Allreduce to send reduction to all ranks.\nOp for details on reduction operators.\n\nExternal links\n\nMPI_Reduce man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Reduce","page":"Collective communication","title":"MPI.Reduce","text":"recvbuf = Reduce(sendbuf, op, root::Integer, comm::Comm)\n\nPerforms elementwise reduction using the operator op on the buffer sendbuf, returning the result recvbuf on the process of rank root, and nothing on non-root processes.\n\nsendbuf can also be a scalar, in which case recvbuf will be a value of the same type.\n\nSee also\n\nReduce! for mutating and in-place operations\nAllreduce!/Allreduce to send reduction to all ranks.\nOp for details on reduction operators.\n\nExternal links\n\nMPI_Reduce man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Allreduce!","page":"Collective communication","title":"MPI.Allreduce!","text":"Allreduce!(sendbuf, recvbuf[, count=length(sendbuf)], op, comm)\nAllreduce!(sendrecvbuf, op, comm)\n\nPerforms elementwise reduction using the operator op on the first count elements of the buffer sendbuf, storing the result in the recvbuf of all processes in the group.\n\nAllreduce! is equivalent to a Reduce! operation followed by a Bcast!, but can lead to better performance.\n\nIf only one sendrecvbuf buffer is provided, then the operation is performed in-place.\n\nSee also\n\nAllreduce, to handle allocation of the output buffer.\nReduce!/Rreduce to send reduction to a single rank.\nOp for details on reduction operators.\n\nExternal links\n\nMPI_Allreduce man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Allreduce","page":"Collective communication","title":"MPI.Allreduce","text":"recvbuf = Allreduce(sendbuf, op, comm)\n\nPerforms elementwise reduction using the operator op on the buffer sendbuf, returning the result in the recvbuf of all processes in the group.\n\nsendbuf can also be a scalar, in which case recvbuf will be a value of the same type.\n\nSee also\n\nAllreduce! for mutating or in-place operations.\nReduce!/Rreduce to send reduction to a single rank.\nOp for details on reduction operators.\n\nExternal links\n\nMPI_Allreduce man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Scan!","page":"Collective communication","title":"MPI.Scan!","text":"Scan!(sendbuf, recvbuf[, count::Integer], op, comm::Comm)\nScan!(buf[, count::Integer], op, comm::Comm)\n\nInclusive prefix reduction (analagous to accumulate in Julia): recvbuf on rank i will contain the the result of reducing sendbuf by op from ranks 0:i.\n\nIf only a single buffer is provided, then operations will be performed in-place in buf.\n\nSee also\n\nScan to handle allocation of the output buffer\nExscan!/Exscan for exclusive scan\n\nExternal links\n\nMPI_Scan man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Scan","page":"Collective communication","title":"MPI.Scan","text":"recvbuf = Scan(sendbuf, op, comm::Comm)\n\nInclusive prefix reduction (analagous to accumulate in Julia): recvbuf on rank i will contain the the result of reducing sendbuf by op from ranks 0:i.\n\nsendbuf can also be a scalar, in which case recvbuf will also be a scalar of the same type.\n\nSee also\n\nScan! for mutating or in-place operations\nExscan!/Exscan for exclusive scan\n\nExternal links\n\nMPI_Scan man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Exscan!","page":"Collective communication","title":"MPI.Exscan!","text":"Exscan!(sendbuf, recvbuf[, count::Integer], op, comm::Comm)\nExscan!(buf[, count::Integer], op, comm::Comm)\n\nExclusive prefix reduction (analagous to accumulate in Julia): recvbuf on rank i will contain the the result of reducing sendbuf by op from ranks 0:i-1. The recvbuf on rank 0 is ignored, and the recvbuf on rank 1 will contain the contents of sendbuf on rank 0.\n\nIf only a single buf is provided, then operations are performed in-place, and buf on rank 0 will remain unchanged.\n\nSee also\n\nExscan to handle allocation of the output buffer\nScan!/Scan for inclusive scan\n\nExternal links\n\nMPI_Exscan man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Exscan","page":"Collective communication","title":"MPI.Exscan","text":"recvbuf = Exscan(sendbuf, op, comm::Comm)\n\nExclusive prefix reduction (analagous to accumulate in Julia): recvbuf on rank i will contain the the result of reducing sendbuf by op from ranks 0:i-1. The recvbuf on rank 0 is undefined, and the recvbuf on rank 1 will contain the contents of sendbuf on rank 0.\n\nSee also\n\nExscan! for mutating and in-place operations\nScan!/Scan for inclusive scan\n\nExternal links\n\nMPI_Scan man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"examples/01-hello/#Hello-world-1","page":"Hello world","title":"Hello world","text":"","category":"section"},{"location":"examples/01-hello/#","page":"Hello world","title":"Hello world","text":"# examples/01-hello.jl\nusing MPI\nMPI.Init()\n\ncomm = MPI.COMM_WORLD\nprint(\"Hello world, I am rank $(MPI.Comm_rank(comm)) of $(MPI.Comm_size(comm))\\n\")\nMPI.Barrier(comm)","category":"page"},{"location":"examples/01-hello/#","page":"Hello world","title":"Hello world","text":"> mpiexec -n 3 julia examples/01-hello.jl\nHello world, I am rank 2 of 3\nHello world, I am rank 0 of 3\nHello world, I am rank 1 of 3","category":"page"},{"location":"#MPI.jl-1","page":"MPI.jl","title":"MPI.jl","text":"","category":"section"},{"location":"#","page":"MPI.jl","title":"MPI.jl","text":"This is a basic Julia wrapper for the portable message passing system Message Passing Interface (MPI). Inspiration is taken from mpi4py, although we generally follow the C and not the C++ MPI API. (The C++ MPI API is deprecated.)","category":"page"}]
}
