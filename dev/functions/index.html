<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Function reference · MPI.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>MPI.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">MPI.jl</a></li><li><a class="toctext" href="../installing/">Installing</a></li><li><a class="toctext" href="../usage/">Usage</a></li><li class="current"><a class="toctext" href>Function reference</a><ul class="internal"><li><a class="toctext" href="#Communicators-1">Communicators</a></li><li><a class="toctext" href="#Administrative-functions-1">Administrative functions</a></li><li><a class="toctext" href="#Point-to-point-communication-1">Point-to-point communication</a></li><li><a class="toctext" href="#Collective-communication-1">Collective communication</a></li><li><a class="toctext" href="#One-sided-communication-1">One-sided communication</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Function reference</a></li></ul><a class="edit-page" href="https://github.com/JuliaParallel/MPI.jl/blob/master/docs/src/functions.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Function reference</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Function-reference-1" href="#Function-reference-1">Function reference</a></h1><p>The following functions are currently wrapped, with the convention: <code>MPI_Fun =&gt; MPI.Fun</code></p><p>Constants like <code>MPI_SUM</code> are wrapped as <code>MPI.SUM</code>.   Note also that arbitrary Julia functions <code>f(x,y)</code> can be passed as reduction operations to the MPI <code>Allreduce</code> and <code>Reduce</code> functions.</p><h2><a class="nav-anchor" id="Communicators-1" href="#Communicators-1">Communicators</a></h2><p>Julia interfaces to the Fortran versions of the MPI functions. Since the C and Fortran communicators are different, if a C communicator is required (e.g., to interface with a C library), this can be achieved with the Fortran to C communicator conversion:</p><pre><code class="language-julia">juliacomm = MPI.COMM_WORLD
ccomm = MPI.CComm(juliacomm)</code></pre><h2><a class="nav-anchor" id="Administrative-functions-1" href="#Administrative-functions-1">Administrative functions</a></h2><table><tr><th style="text-align: right">Julia Function (assuming <code>import MPI</code>)</th><th style="text-align: right">Fortran Function</th></tr><tr><td style="text-align: right"><a href="#MPI.Abort"><code>MPI.Abort</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Abort.3.php"><code>MPI_Abort</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Comm_dup</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Comm_dup.3.php"><code>MPI_Comm_dup</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Comm_free</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Comm_free.3.php"><code>MPI_Comm_free</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Comm_get_parent</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Comm_get_parent.3.php"><code>MPI_Comm_get_parent</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Comm_rank"><code>MPI.Comm_rank</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Comm_rank.3.php"><code>MPI_Comm_rank</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Comm_size"><code>MPI.Comm_size</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Comm_size.3.php"><code>MPI_Comm_size</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Comm_spawn</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Comm_spawn.3.php"><code>MPI_Comm_spawn</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Finalize"><code>MPI.Finalize</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Finalize.3.php"><code>MPI_Finalize</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Finalized"><code>MPI.Finalized</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Finalized.3.php"><code>MPI_Finalized</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Get_address</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Get_address.3.php"><code>MPI_Get_address</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Init"><code>MPI.Init</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Init.3.php"><code>MPI_Init</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Initialized"><code>MPI.Initialized</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Initialized.3.php"><code>MPI_Initialized</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Intercomm_merge</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Intercomm_merge.3.php"><code>MPI_Intercomm_merge</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.mpitype</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Type_create_struct.3.php"><code>MPI_Type_create_struct</code></a>/<a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Type_commit.3.php"><code>MPI_Type_commit</code></a></td></tr></table><div class="admonition note"><div class="admonition-title">Note</div><div class="admonition-text"><p><code>mpitype</code> is not strictly a wrapper for <code>MPI_Type_create_struct</code> and <code>MPI_Type_commit</code>, it also is an accessor for previously created types.</p></div></div><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Abort" href="#MPI.Abort"><code>MPI.Abort</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Abort(comm::Comm, errcode::Integer)</code></pre><p>Make a “best attempt” to abort all tasks in the group of <code>comm</code>. This function does not require that the invoking environment take any action with the error code. However, a Unix or POSIX environment should handle this as a return errorcode from the main program.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L317-L323">source</a></section><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Comm_dup</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Comm_free</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Comm_get_parent</code>. Check Documenter&#39;s build log for details.</p></div></div><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Comm_rank" href="#MPI.Comm_rank"><code>MPI.Comm_rank</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Comm_rank(comm:Comm)</code></pre><p>The rank of the process in the particular communicator’s group. </p><p>Returns an integer in the range <code>0:MPI.Comm_size()-1</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L367-L373">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Comm_size" href="#MPI.Comm_size"><code>MPI.Comm_size</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Comm_size(comm:Comm)</code></pre><p>The number of processes involved in communicator.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L381-L385">source</a></section><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Comm_spawn</code>. Check Documenter&#39;s build log for details.</p></div></div><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Finalize" href="#MPI.Finalize"><code>MPI.Finalize</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Finalize()</code></pre><p>Marks MPI state for cleanup. This should be called after <a href="#MPI.Init"><code>Init</code></a>, at most once, and no further MPI calls (other than <a href="#MPI.Initialized"><code>Initialized</code></a> or <a href="#MPI.Finalized"><code>Finalized</code></a>) should be made after it is called.</p><p>Note that this does not correspond exactly to <code>MPI_FINALIZE</code> in the MPI specification. In particular:</p><ul><li><p>It may not finalize MPI immediately. Julia will wait until all MPI-related objects are garbage collected before finalizing MPI. As a result, <a href="#MPI.Finalized"><code>Finalized()</code></a> may return <code>false</code> after <code>Finalize()</code> has been called. See <a href="../usage/#Finalizers-1">Finalizers</a> for more details.</p></li><li><p>It is optional: <a href="#MPI.Init"><code>Init</code></a> will automatically insert a hook to finalize MPI when Julia exits.</p></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L273-L290">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Finalized" href="#MPI.Finalized"><code>MPI.Finalized</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Finalized()</code></pre><p>Returns <code>true</code> if <a href="#MPI.Finalize"><code>MPI.Finalize</code></a> has completed, <code>false</code> otherwise. </p><p>It is safe to call before <a href="#MPI.Init"><code>MPI.Init</code></a> and after <a href="#MPI.Finalize"><code>MPI.Finalize</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L343-L349">source</a></section><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Get_address</code>. Check Documenter&#39;s build log for details.</p></div></div><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Init" href="#MPI.Init"><code>MPI.Init</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Init()</code></pre><p>Initialize MPI in the current process.</p><p>All MPI programs must contain exactly one call to <code>MPI.Init()</code>.</p><p>The only MPI functions that may be called before <code>MPI.Init()</code> are <a href="#MPI.Initialized"><code>MPI.Initialized</code></a> and <a href="#MPI.Finalized"><code>MPI.Finalized</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L255-L264">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Initialized" href="#MPI.Initialized"><code>MPI.Initialized</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Initialized()</code></pre><p>Returns <code>true</code> if <a href="#MPI.Init"><code>MPI.Init</code></a> has been called, <code>false</code> otherwise. </p><p>It is unaffected by <a href="#MPI.Finalize"><code>MPI.Finalize</code></a>, and is one of the few functions that may be called before <a href="#MPI.Init"><code>MPI.Init</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L329-L336">source</a></section><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Intercomm_merge</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.mpitype</code>. Check Documenter&#39;s build log for details.</p></div></div><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.refcount_inc-Tuple{}" href="#MPI.refcount_inc-Tuple{}"><code>MPI.refcount_inc</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-julia">refcount_inc()</code></pre><p>Increment the MPI reference counter. This should be called at initialization of any object which calls an MPI routine in its finalizer. A matching <a href="#MPI.refcount_dec-Tuple{}"><code>refcount_dec</code></a> should be added to the finalizer.</p><p>For more details, see <a href="../usage/#Finalizers-1">Finalizers</a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L226-L234">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.refcount_dec-Tuple{}" href="#MPI.refcount_dec-Tuple{}"><code>MPI.refcount_dec</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-julia">refcount_dec()</code></pre><p>Decrement the MPI reference counter. This should be added after an MPI call in an object finalizer, with a matching <a href="#MPI.refcount_inc-Tuple{}"><code>refcount_inc</code></a> when the object is initialized.</p><p>For more details, see <a href="../usage/#Finalizers-1">Finalizers</a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L239-L246">source</a></section><h2><a class="nav-anchor" id="Point-to-point-communication-1" href="#Point-to-point-communication-1">Point-to-point communication</a></h2><table><tr><th style="text-align: right">Julia Function (assuming <code>import MPI</code>)</th><th style="text-align: right">Fortran Function</th></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Cancel!</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Cancel.3.php"><code>MPI_Cancel</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Get_count</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Get_count.3.php"><code>MPI_Get_count</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Iprobe</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Iprobe.3.php"><code>MPI_Iprobe</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Irecv!"><code>MPI.Irecv!</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Irecv.3.php"><code>MPI_Irecv</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Isend"><code>MPI.Isend</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Isend.3.php"><code>MPI_Isend</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Probe</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Probe.3.php"><code>MPI_Probe</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Recv!"><code>MPI.Recv!</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Recv.3.php"><code>MPI_Recv</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Send"><code>MPI.Send</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Send.3.php"><code>MPI_Send</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Test!</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Test.3.php"><code>MPI_Test</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Testall!</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Testall.3.php"><code>MPI_Testall</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Testany!</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Testany.3.php"><code>MPI_Testany</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Testsome!</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Testsome.3.php"><code>MPI_Testsome</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Wait!"><code>MPI.Wait!</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Wait.3.php"><code>MPI_Wait</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Waitall!"><code>MPI.Waitall!</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Waitall.3.php"><code>MPI_Waitall</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Waitany!"><code>MPI.Waitany!</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Waitany.3.php"><code>MPI_Waitany</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Waitsome!</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Waitsome.3.php"><code>MPI_Waitsome</code></a></td></tr></table><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Cancel!</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Get_count</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Iprobe</code>. Check Documenter&#39;s build log for details.</p></div></div><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Irecv!" href="#MPI.Irecv!"><code>MPI.Irecv!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Irecv!(buf::MPIBuffertype{T}, count::Integer, src::Integer, tag::Integer,
       comm::Comm) where T</code></pre><p>Starts a nonblocking receive of up to <code>count</code> elements into <code>buf</code> from MPI rank <code>src</code> of communicator <code>comm</code> using with the message tag <code>tag</code></p><p>Returns the communication <code>Request</code> for the nonblocking receive.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L738-L746">source</a><div><div><pre><code class="language-none">Irecv!(buf::Array{T}, src::Integer, tag::Integer, comm::Comm) where T</code></pre><p>Starts a nonblocking receive into <code>buf</code> from MPI rank <code>src</code> of communicator <code>comm</code> using with the message tag <code>tag</code></p><p>Returns the communication <code>Request</code> for the nonblocking receive.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L757-L764">source</a><div><div><pre><code class="language-none">Irecv!(buf::SubArray{T}, src::Integer, tag::Integer, comm::Comm) where T</code></pre><p>Starts a nonblocking receive into <code>SubArray</code> <code>buf</code> from MPI rank <code>src</code> of communicator <code>comm</code> using with the message tag <code>tag</code>. Note that <code>buf</code> must be contiguous.</p><p>Returns the communication <code>Request</code> for the nonblocking receive.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L770-L778">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Isend" href="#MPI.Isend"><code>MPI.Isend</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Isend(buf::MPIBuffertype{T}, count::Integer, dest::Integer, tag::Integer,
      comm::Comm) where T</code></pre><p>Starts a nonblocking send of <code>count</code> elements of <code>buf</code> to MPI rank <code>dest</code> of communicator <code>comm</code> using with the message tag <code>tag</code></p><p>Returns the commication <code>Request</code> for the nonblocking send.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L607-L615">source</a><div><div><pre><code class="language-none">Isend(buf::Array{T}, dest::Integer, tag::Integer, comm::Comm) where T</code></pre><p>Starts a nonblocking send of <code>buf</code> to MPI rank <code>dest</code> of communicator <code>comm</code> using with the message tag <code>tag</code></p><p>Returns the commication <code>Request</code> for the nonblocking send.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L626-L633">source</a><div><div><pre><code class="language-none">Isend(buf::SubArray{T}, dest::Integer, tag::Integer, comm::Comm) where T</code></pre><p>Starts a nonblocking send of <code>SubArray</code> <code>buf</code> to MPI rank <code>dest</code> of communicator <code>comm</code> using with the message tag <code>tag</code>. Note that the <code>buf</code> must be contiguous.</p><p>Returns the commication <code>Request</code> for the nonblocking send.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L638-L645">source</a><div><div><pre><code class="language-none">Isend(obj::T, dest::Integer, tag::Integer, comm::Comm) where T</code></pre><p>Starts a nonblocking send of <code>obj</code> to MPI rank <code>dest</code> of communicator <code>comm</code> using with the message tag <code>tag</code>.</p><p>Returns the commication <code>Request</code> for the nonblocking send.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L652-L659">source</a></section><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Probe</code>. Check Documenter&#39;s build log for details.</p></div></div><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Recv!" href="#MPI.Recv!"><code>MPI.Recv!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Recv!(buf::MPIBuffertype{T}, count::Integer, src::Integer, tag::Integer,
      comm::Comm) where T</code></pre><p>Completes a blocking receive of up to <code>count</code> elements into <code>buf</code> from MPI rank <code>src</code> of communicator <code>comm</code> using with the message tag <code>tag</code></p><p>Returns the <code>Status</code> of the receive</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L678-L686">source</a><div><div><pre><code class="language-none">Recv!(buf::Array{T}, src::Integer, tag::Integer, comm::Comm) where T</code></pre><p>Completes a blocking receive into <code>buf</code> from MPI rank <code>src</code> of communicator <code>comm</code> using with the message tag <code>tag</code></p><p>Returns the <code>Status</code> of the receive</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L698-L705">source</a><div><div><pre><code class="language-none">Recv!(buf::SubArray{T}, src::Integer, tag::Integer, comm::Comm) where T</code></pre><p>Completes a blocking receive into <code>SubArray</code> <code>buf</code> from MPI rank <code>src</code> of communicator <code>comm</code> using with the message tag <code>tag</code>. Note that <code>buf</code> must be contiguous.</p><p>Returns the <code>Status</code> of the receive</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L710-L718">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Send" href="#MPI.Send"><code>MPI.Send</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Send(buf::MPIBuffertype{T}, count::Integer, dest::Integer, tag::Integer,
     comm::Comm) where T</code></pre><p>Complete a blocking send of <code>count</code> elements of <code>buf</code> to MPI rank <code>dest</code> of communicator <code>comm</code> using with the message tag <code>tag</code></p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L549-L555">source</a><div><div><pre><code class="language-none">Send(buf::Array{T}, dest::Integer, tag::Integer, comm::Comm) where T</code></pre><p>Complete a blocking send of <code>buf</code> to MPI rank <code>dest</code> of communicator <code>comm</code> using with the message tag <code>tag</code></p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L564-L569">source</a><div><div><pre><code class="language-none">Send(buf::SubArray{T}, dest::Integer, tag::Integer, comm::Comm) where T</code></pre><p>Complete a blocking send of <code>SubArray</code> <code>buf</code> to MPI rank <code>dest</code> of communicator <code>comm</code> using with the message tag <code>tag</code>. Note that the <code>buf</code> must be contiguous.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L574-L579">source</a><div><div><pre><code class="language-none">Send(obj::T, dest::Integer, tag::Integer, comm::Comm) where T</code></pre><p>Complete s blocking send of <code>obj</code> to MPI rank <code>dest</code> of communicator <code>comm</code> using with the message tag <code>tag</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L585-L590">source</a></section><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Test!</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Testall!</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Testany!</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Testsome!</code>. Check Documenter&#39;s build log for details.</p></div></div><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Wait!" href="#MPI.Wait!"><code>MPI.Wait!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Wait!(req::Request)</code></pre><p>Wait on the request <code>req</code> to be complete. Returns the <code>Status</code> of the request.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L796-L800">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Waitall!" href="#MPI.Waitall!"><code>MPI.Waitall!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Waitall!(reqs::Array{Request,1})</code></pre><p>Wait on all the requests in the array <code>reqs</code> to be complete. Returns an arrays of the all the requests statuses.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L821-L826">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Waitany!" href="#MPI.Waitany!"><code>MPI.Waitany!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Waitany!(reqs::Array{Request,1})</code></pre><p>Wait on any the requests in the array <code>reqs</code> to be complete. Returns the index of the completed request and its <code>Status</code> as a tuple.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L870-L875">source</a></section><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Waitsome!</code>. Check Documenter&#39;s build log for details.</p></div></div><h2><a class="nav-anchor" id="Collective-communication-1" href="#Collective-communication-1">Collective communication</a></h2><table><tr><th style="text-align: right">Non-Allocating Julia Function</th><th style="text-align: right">Allocating Julia Function</th><th style="text-align: right">Fortran Function</th><th style="text-align: right">Supports <code>MPI_IN_PLACE</code></th></tr><tr><td style="text-align: right"><a href="#MPI.Allgather!"><code>MPI.Allgather!</code></a></td><td style="text-align: right"><a href="#MPI.Allgather"><code>MPI.Allgather</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Allgather.3.php"><code>MPI_Allgather</code></a></td><td style="text-align: right">✅</td></tr><tr><td style="text-align: right"><a href="#MPI.Allgatherv!"><code>MPI.Allgatherv!</code></a></td><td style="text-align: right"><a href="#MPI.Allgatherv"><code>MPI.Allgatherv</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Allgatherv.3.php"><code>MPI_Allgatherv</code></a></td><td style="text-align: right">✅</td></tr><tr><td style="text-align: right"><a href="#MPI.Allreduce!"><code>MPI.Allreduce!</code></a></td><td style="text-align: right"><a href="#MPI.Allreduce"><code>MPI.Allreduce</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Allreduce.3.php"><code>MPI_Allreduce</code></a></td><td style="text-align: right">✅</td></tr><tr><td style="text-align: right"><a href="#MPI.Alltoall!"><code>MPI.Alltoall!</code></a></td><td style="text-align: right"><a href="#MPI.Alltoall"><code>MPI.Alltoall</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Alltoall.3.php"><code>MPI_Alltoall</code></a></td><td style="text-align: right">✅</td></tr><tr><td style="text-align: right"><a href="#MPI.Alltoallv!"><code>MPI.Alltoallv!</code></a></td><td style="text-align: right"><a href="@ref"><code>MPI.Alltoallv</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Alltoallv.3.php"><code>MPI_Alltoallv</code></a></td><td style="text-align: right">❌</td></tr><tr><td style="text-align: right">–</td><td style="text-align: right"><a href="#MPI.Barrier"><code>MPI.Barrier</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Barrier.3.php"><code>MPI_Barrier</code></a></td><td style="text-align: right">❌</td></tr><tr><td style="text-align: right"><a href="#MPI.Bcast!"><code>MPI.Bcast!</code></a></td><td style="text-align: right"><a href="#MPI.Bcast!"><code>MPI.Bcast!</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Bcast.3.php"><code>MPI_Bcast</code></a></td><td style="text-align: right">❌</td></tr><tr><td style="text-align: right">–</td><td style="text-align: right"><a href="@ref"><code>MPI.Exscan</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Exscan.3.php"><code>MPI_Exscan</code></a></td><td style="text-align: right">❌</td></tr><tr><td style="text-align: right"><a href="#MPI.Gather!"><code>MPI.Gather!</code></a></td><td style="text-align: right"><a href="#MPI.Gather"><code>MPI.Gather</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Gather.3.php"><code>MPI_Gather</code></a></td><td style="text-align: right"><a href="@ref"><code>Gather_in_place!</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Gatherv!"><code>MPI.Gatherv!</code></a></td><td style="text-align: right"><a href="#MPI.Gatherv"><code>MPI.Gatherv</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Gatherv.3.php"><code>MPI_Gatherv</code></a></td><td style="text-align: right"><a href="@ref"><code>Gatherv_in_place!</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Reduce!"><code>MPI.Reduce!</code></a></td><td style="text-align: right"><a href="#MPI.Reduce"><code>MPI.Reduce</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Reduce.3.php"><code>MPI_Reduce</code></a></td><td style="text-align: right"><a href="@ref"><code>Reduce_in_place!</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Scan</code></a></td><td style="text-align: right"><a href="@ref"><code>MPI.Scan</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Scan.3.php"><code>MPI_Scan</code></a></td><td style="text-align: right">missing</td></tr><tr><td style="text-align: right"><a href="#MPI.Scatter!"><code>MPI.Scatter!</code></a></td><td style="text-align: right"><a href="#MPI.Scatter"><code>MPI.Scatter</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Scatter.3.php"><code>MPI_Scatter</code></a></td><td style="text-align: right"><a href="@ref"><code>Scatter_in_place!</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Scatterv!"><code>MPI.Scatterv!</code></a></td><td style="text-align: right"><a href="#MPI.Scatterv"><code>MPI.Scatterv</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Scatterv.3.php"><code>MPI_Scatterv</code></a></td><td style="text-align: right"><a href="@ref"><code>Scatterv_in_place!</code></a></td></tr></table><p>The non-allocating Julia functions map directly to the corresponding MPI operations, after asserting that the size of the output buffer is sufficient to store the result.</p><p>The allocating Julia functions allocate an output buffer and then call the non-allocating method.</p><p>All-to-all collective communications support in place operations by passing <code>MPI.IN_PLACE</code> with the same syntax documented by MPI. One-to-All communications support it by calling the function <code>*_in_place!</code>, calls the MPI functions with the right syntax on root and non root process.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Allgather!" href="#MPI.Allgather!"><code>MPI.Allgather!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Allgather!(sendbuf, recvbuf, count, comm)</code></pre><p>Each process sends the first <code>count</code> elements of <code>sendbuf</code> to the other processes, who store the results in rank order into <code>recvbuf</code>.</p><p>If <code>sendbuf==MPI.IN_PLACE</code> the input data is assumed to be in the area of <code>recvbuf</code> where the process would receive it&#39;s own contribution.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L1463-L1473">source</a><div><div><pre><code class="language-none">Allgather!(buf, count, comm)</code></pre><p>Equivalent to <code>Allgather!(MPI.IN_PLACE, buf, count, comm)</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L1483-L1487">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Allgather" href="#MPI.Allgather"><code>MPI.Allgather</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Allgather(sendbuf, count, comm)</code></pre><p>Each process sends the first <code>count</code> elements of <code>sendbuf</code> to the other processes, who store the results in rank order allocating the output buffer.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L1493-L1499">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Allgatherv!" href="#MPI.Allgatherv!"><code>MPI.Allgatherv!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Allgatherv!(sendbuf, recvbuf, counts, comm)</code></pre><p>Each process sends the first <code>counts[rank]</code> elements of the buffer <code>sendbuf</code> to all other process. Each process stores the received data in rank order in the buffer <code>recvbuf</code>.</p><p>if <code>sendbuf==MPI.IN_PLACE</code> every process takes the data to be sent is taken from the interval of <code>recvbuf</code> where it would store it&#39;s own data.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L1592-L1601">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Allgatherv" href="#MPI.Allgatherv"><code>MPI.Allgatherv</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Allgatherv(sendbuf, counts, comm)</code></pre><p>Each process sends the first <code>counts[rank]</code> elements of the buffer <code>sendbuf</code> to all other process. Each process allocates an output buffer and stores the received data in rank order.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L1613-L1619">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Allreduce!" href="#MPI.Allreduce!"><code>MPI.Allreduce!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Allreduce!(sendbuf, recvbuf[, count=length(sendbuf)], op, comm)</code></pre><p>Performs <code>op</code> reduction on the first <code>count</code> elements of the buffer <code>sendbuf</code> storing the result in the <code>recvbuf</code> of all processes in the group.</p><p>All-reduce is equivalent to a <a href="#MPI.Reduce!"><code>Reduce!</code></a> operation followed by a <a href="#MPI.Bcast!"><code>Bcast!</code></a>, but can lead to better performance.</p><p>If <code>sendbuf==MPI.IN_PLACE</code> the data is read from <code>recvbuf</code> and then overwritten with the results.</p><p>To handle allocation of the output buffer, see <a href="#MPI.Allreduce"><code>Allreduce</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L1151-L1165">source</a><div><div><pre><code class="language-none">Allreduce!(buf, op, comm)</code></pre><p>Performs <code>op</code> reduction in place on the buffer <code>sendbuf</code>, overwriting it with the results on all the processes in the group.</p><p>Equivalent to calling <code>Allreduce!(MPI.IN_PLACE, buf, op, comm)</code></p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L1185-L1192">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Allreduce" href="#MPI.Allreduce"><code>MPI.Allreduce</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Allreduce(sendbuf, op, comm)</code></pre><p>Performs <code>op</code> reduction on the buffer <code>sendbuf</code>, allocating and returning the output buffer in all processes of the group.</p><p>To specify the output buffer or perform the operation in pace, see <a href="#MPI.Allreduce!"><code>Allreduce!</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L1197-L1204">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Alltoall!" href="#MPI.Alltoall!"><code>MPI.Alltoall!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Alltoall!(sendbuf, recvbuf, count, comm)</code></pre><p>Every process divides the buffer <code>sendbuf</code> into <code>Comm_size(comm)</code> chunks of length <code>count</code>, sending the <code>j</code>-th chunk to the <code>j</code>-th process. Every process stores the data received from the <code>j</code>-th process in the <code>j</code>-th chunk of the buffer <code>recvbuf</code>.</p><pre><code class="language-none">rank    send buf                        recv buf
----    --------                        --------
 0      a,b,c,d,e,f       Alltoall      a,b,A,B,α,β
 1      A,B,C,D,E,F  ----------------&gt;  c,d,C,D,γ,ψ
 2      α,β,γ,ψ,η,ν                     e,f,E,F,η,ν</code></pre><p>If <code>sendbuf==MPI.IN_PLACE</code>, data is sent from the <code>recvbuf</code> and then overwritten.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L1626-L1644">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Alltoall" href="#MPI.Alltoall"><code>MPI.Alltoall</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Alltoall(sendbuf, count, comm)</code></pre><p>Every process divides the buffer <code>sendbuf</code> into <code>Comm_size(comm)</code> chunks of length <code>count</code>, sending the <code>j</code>-th chunk to the <code>j</code>-th process. Every process allocates the output buffer and stores the data received from the <code>j</code>-th process in the <code>j</code>-th chunk.</p><pre><code class="language-none">rank    send buf                        recv buf
----    --------                        --------
 0      a,b,c,d,e,f       Alltoall      a,b,A,B,α,β
 1      A,B,C,D,E,F  ----------------&gt;  c,d,C,D,γ,ψ
 2      α,β,γ,ψ,η,ν                     e,f,E,F,η,ν</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L1653-L1668">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Alltoallv!" href="#MPI.Alltoallv!"><code>MPI.Alltoallv!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Alltoallv!(sendbuf::T, recvbuf::T, scounts, rcounts, comm)</code></pre><p><code>MPI.IN_PLACE</code> is not supported for this operation.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L1675-L1679">source</a></section><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Alltoallv</code>. Check Documenter&#39;s build log for details.</p></div></div><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Barrier" href="#MPI.Barrier"><code>MPI.Barrier</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Barrier(comm::Comm)</code></pre><p>Blocks until <code>comm</code> is synchronized.</p><p>If <code>comm</code> is an intracommunicator, then it blocks until all members of the group have called it.</p><p>If <code>comm</code> is an intercommunicator, then it blocks until all members of the other group have called it.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L973-L981">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Bcast!" href="#MPI.Bcast!"><code>MPI.Bcast!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Bcast!(buf[, count=length(buf)], root, comm::Comm)</code></pre><p>Broadcast the first <code>count</code> elements of the buffer <code>buf</code> from <code>root</code> to all processes.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L986-L990">source</a></section><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Bcast!</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Exscan</code>. Check Documenter&#39;s build log for details.</p></div></div><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Gather!" href="#MPI.Gather!"><code>MPI.Gather!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Gather!(sendbuf, recvbuf, count, root, comm)</code></pre><p>Each process sends the first <code>count</code> elements of the buffer <code>sendbuf</code> to the <code>root</code> process. The <code>root</code> process stores elements in rank order in the buffer buffer <code>recvbuf</code>.</p><p><code>count</code> should be the same for all processes. If the number of elements varies between processes, use <a href="#MPI.Gatherv!"><code>Gatherv!</code></a> instead.</p><p>To perform the reduction in place refer to <a href="#MPI.Gather_in_place!"><code>Gather_in_place!</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L1379-L1390">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Gather" href="#MPI.Gather"><code>MPI.Gather</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Gather(sendbuf[, count=length(sendbuf)], root, comm)</code></pre><p>Each process sends the first <code>count</code> elements of the buffer <code>sendbuf</code> to the <code>root</code> process. The <code>root</code> allocates the output buffer and stores elements in rank order.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L1402-L1408">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Gather_in_place!" href="#MPI.Gather_in_place!"><code>MPI.Gather_in_place!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Gather_in_place!(buf, count, root, comm)</code></pre><p>Each process sends the first <code>count</code> elements of the buffer <code>buf</code> to the <code>root</code> process. The <code>root</code> process stores elements in rank order in the buffer buffer <code>buf</code>, sending no data to itself.</p><p>This is functionally equivalent to calling</p><pre><code class="language-none">if root == MPI.Comm_rank(comm)
    Gather!(MPI.IN_PLACE, buf, count, root, comm)
else
    Gather!(buf, C_NULL, count, root, comm)
end</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L1432-L1447">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Gatherv!" href="#MPI.Gatherv!"><code>MPI.Gatherv!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Gatherv!(sendbuf, recvbuf, counts, root, comm)</code></pre><p>Each process sends the first <code>counts[rank]</code> elements of the buffer <code>sendbuf</code> to the <code>root</code> process. The <code>root</code> stores elements in rank order in the buffer <code>recvbuf</code>.</p><p>To perform the reduction in place refer to <a href="#MPI.Gatherv_in_place!"><code>Gatherv_in_place!</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L1521-L1529">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Gatherv" href="#MPI.Gatherv"><code>MPI.Gatherv</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Gatherv(sendbuf, counts, root, comm)</code></pre><p>Each process sends the first <code>counts[rank]</code> elements of the buffer <code>sendbuf</code> to the <code>root</code> process. The <code>root</code> allocates the output buffer and stores elements in rank order.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L1542-L1548">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Gatherv_in_place!" href="#MPI.Gatherv_in_place!"><code>MPI.Gatherv_in_place!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Gatherv_in_place!(buf, counts, root, comm)</code></pre><p>Each process sends the first <code>counts[rank]</code> elements of the buffer <code>buf</code> to the <code>root</code> process. The <code>root</code> allocates the output buffer and stores elements in rank order.</p><p>This is functionally equivalent to calling</p><pre><code class="language-none">if root == MPI.Comm_rank(comm)
    Gatherv!(MPI.IN_PLACE, buf, counts, root, comm)
else
    Gatherv!(buf, C_NULL, counts, root, comm)
end</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L1556-L1571">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Reduce!" href="#MPI.Reduce!"><code>MPI.Reduce!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Reduce!(sendbuf, recvbuf[, count=length(sendbuf)], op, root, comm)</code></pre><p>Performs <code>op</code> reduction on the first <code>count</code> elements of the  buffer <code>sendbuf</code> and stores the result in <code>recvbuf</code> on the process of rank <code>root</code>.</p><p>On non-root processes <code>recvbuf</code> is ignored.</p><p>To perform the reduction in place, see <a href="#MPI.Reduce_in_place!"><code>Reduce_in_place!</code></a>.</p><p>To handle allocation of the output buffer, see <a href="#MPI.Reduce"><code>Reduce</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L1034-L1045">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Reduce" href="#MPI.Reduce"><code>MPI.Reduce</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Reduce(sendbuf, count, op, root, comm)</code></pre><p>Performs <code>op</code> reduction on the buffer <code>sendbuf</code> and stores the result in an output buffer allocated on the process of rank <code>root</code>. An empty array will be returned on all other processes.</p><p>To specify the output buffer, see <a href="#MPI.Reduce!"><code>Reduce!</code></a>.</p><p>To perform the reduction in place, see <a href="#MPI.Reduce_in_place!"><code>Reduce_in_place!</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L1070-L1080">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Reduce_in_place!" href="#MPI.Reduce_in_place!"><code>MPI.Reduce_in_place!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Reduce_in_place!(buf, count, op, root, comm)</code></pre><p>Performs <code>op</code> reduction on the first <code>count</code> elements of the buffer <code>buf</code> and stores the result on <code>buf</code> of the <code>root</code> process in the group.</p><p>This is equivalent to calling</p><pre><code class="language-julia">if root == MPI.Comm_rank(comm)
    Reduce!(MPI.IN_PLACE, buf, count, root, comm)
else
    Reduce!(buf, C_NULL, count, root, comm)
end</code></pre><p>To handle allocation of the output buffer, see <a href="#MPI.Reduce"><code>Reduce</code></a>.</p><p>To specify a separate output buffer, see <a href="#MPI.Reduce!"><code>Reduce!</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L1107-L1125">source</a></section><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Scan</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Scan</code>. Check Documenter&#39;s build log for details.</p></div></div><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Scatter!" href="#MPI.Scatter!"><code>MPI.Scatter!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Scatter!(sendbuf, recvbuf, count, root, comm)</code></pre><p>Splits the buffer <code>sendbuf</code> in the <code>root</code> process into <code>Comm_size(comm)</code> chunks and sends the j-th chunk to the process of rank j into the <code>recvbuf</code> buffer, which must be of length at least <code>count</code>.</p><p><code>count</code> should be the same for all processes. If the number of elements varies between processes, use <a href="#MPI.Scatter!"><code>Scatter!</code></a> instead.</p><p>To perform the reduction in place, see <a href="#MPI.Scatter_in_place!"><code>Scatter_in_place!</code></a>.</p><p>To handle allocation of the output buffer, see <a href="#MPI.Scatter"><code>Scatter</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L1234-L1247">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Scatter" href="#MPI.Scatter"><code>MPI.Scatter</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Scatter(sendbuf, count, root, comm)</code></pre><p>Splits the buffer <code>sendbuf</code> in the <code>root</code> process into <code>Comm_size(comm)</code> chunks and sends the j-th chunk to the process of rank j, allocating the output buffer.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L1298-L1303">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Scatter_in_place!" href="#MPI.Scatter_in_place!"><code>MPI.Scatter_in_place!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Scatter_in_place!(buf, count, root, comm)</code></pre><p>Splits the buffer <code>buf</code> in the <code>root</code> process into <code>Comm_size(comm)</code> chunks and sends the j-th chunk to the process of rank j. No data is sent to the <code>root</code> process.</p><p>This is functionally equivalent to calling</p><pre><code class="language-none">if root == MPI.Comm_rank(comm)
    Scatter!(buf, MPI.IN_PLACE, count, root, comm)
else
    Scatter!(C_NULL, buf, count, root, comm)
end</code></pre><p>To specify a separate output buffer, see <a href="#MPI.Scatter!"><code>Scatter!</code></a>.</p><p>To handle allocation of the output buffer, see <a href="#MPI.Scatter"><code>Scatter</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L1261-L1280">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Scatterv!" href="#MPI.Scatterv!"><code>MPI.Scatterv!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Scatterv!(sendbuf, recvbuf, counts, root, comm)</code></pre><p>Splits the buffer <code>sendbuf</code> in the <code>root</code> process into <code>Comm_size(comm)</code> chunks of length <code>counts[j]</code> and sends the j-th chunk to the process of rank j into the <code>recvbuf</code> buffer, which must be of length at least <code>count</code>.</p><p>To perform the reduction in place refer to <a href="#MPI.Scatterv_in_place!"><code>Scatterv_in_place!</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L1310-L1318">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Scatterv" href="#MPI.Scatterv"><code>MPI.Scatterv</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Scatterv(sendbuf, counts, root, comm)</code></pre><p>Splits the buffer <code>sendbuf</code> in the <code>root</code> process into <code>Comm_size(comm)</code> chunks of length <code>counts[j]</code> and sends the j-th chunk to the process of rank j, which allocates the output buffer</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L1330-L1336">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Scatterv_in_place!" href="#MPI.Scatterv_in_place!"><code>MPI.Scatterv_in_place!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Scatterv_in_place(buf, counts, root, comm)</code></pre><p>Splits the buffer <code>buf</code> in the <code>root</code> process into <code>Comm_size(comm)</code> chunks of length <code>counts[j]</code> and sends the j-th chunk to the process of rank j into the <code>buf</code> buffer, which must be of length at least <code>count</code>. The <code>root</code> process sends nothing to itself.</p><p>This is functionally equivalent to calling</p><pre><code class="language-none">if root == MPI.Comm_rank(comm)
    Scatterv!(buf, MPI.IN_PLACE, counts, root, comm)
else
    Scatterv!(C_NULL, buf, counts, root, comm)
end</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/3f1aa8f45afb27c96ce4b31cc4b655315591331a/src/mpi-base.jl#L1343-L1359">source</a></section><h2><a class="nav-anchor" id="One-sided-communication-1" href="#One-sided-communication-1">One-sided communication</a></h2><table><tr><th style="text-align: right">Julia Function (assuming <code>import MPI</code>)</th><th style="text-align: right">Fortran Function</th></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_create</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_create.3.php"><code>MPI_Win_create</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_create_dynamic</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_create_dynamic.3.php"><code>MPI_Win_create_dynamic</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_allocate_shared</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_allocate_shared.3.php"><code>MPI_Win_allocate_shared</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_shared_query</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_shared_query.3.php"><code>MPI_Win_shared_query</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_attach</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_attach.3.php"><code>MPI_Win_attach</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_detach</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_detach.3.php"><code>MPI_Win_detach</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_fence</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_fence.3.php"><code>MPI_Win_fence</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_flush</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_flush.3.php"><code>MPI_Win_flush</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_free</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_free.3.php"><code>MPI_Win_free</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_sync</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_sync.3.php"><code>MPI_Win_sync</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_lock</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_lock.3.php"><code>MPI_Win_lock</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_unlock</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_unlock.3.php"><code>MPI_Win_unlock</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Get</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Get.3.php"><code>MPI_Get</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Put</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Put.3.php"><code>MPI_Put</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Fetch_and_op</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Fetch_and_op.3.php"><code>MPI_Fetch_and_op</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Accumulate</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Accumulate.3.php"><code>MPI_Accumulate</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Get_accumulate</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Get_accumulate.3.php"><code>MPI_Get_accumulate</code></a></td></tr></table><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Win_create</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Win_create_dynamic</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Win_allocate_shared</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Win_shared_query</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Win_attach</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Win_detach</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Win_fence</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Win_flush</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Win_free</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Win_sync</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Win_lock</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Win_unlock</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Get</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Put</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Fetch_and_op</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Accumulate</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Get_accumulate</code>. Check Documenter&#39;s build log for details.</p></div></div><footer><hr/><a class="previous" href="../usage/"><span class="direction">Previous</span><span class="title">Usage</span></a></footer></article></body></html>
